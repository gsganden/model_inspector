[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Model Inspector",
    "section": "",
    "text": "model_inspector aims to help you train better scikit-learn-compatible models by providing insights into their behavior."
  },
  {
    "objectID": "index.html#use",
    "href": "index.html#use",
    "title": "Model Inspector",
    "section": "Use",
    "text": "Use\nTo use model_inspector, you create an Inspector object from a scikit-learn model, a feature DataFrame X, and a target Series y. Typically you will want to create it on held-out data, as shown below.\n\nimport sklearn.datasets\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\nfrom model_inspector import get_inspector\n\n\nX, y = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n\n\nX\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n437\n0.041708\n0.050680\n0.019662\n0.059744\n-0.005697\n-0.002566\n-0.028674\n-0.002592\n0.031193\n0.007207\n\n\n438\n-0.005515\n0.050680\n-0.015906\n-0.067642\n0.049341\n0.079165\n-0.028674\n0.034309\n-0.018114\n0.044485\n\n\n439\n0.041708\n0.050680\n-0.015906\n0.017293\n-0.037344\n-0.013840\n-0.024993\n-0.011080\n-0.046883\n0.015491\n\n\n440\n-0.045472\n-0.044642\n0.039062\n0.001215\n0.016318\n0.015283\n-0.028674\n0.026560\n0.044529\n-0.025930\n\n\n441\n-0.045472\n-0.044642\n-0.073030\n-0.081413\n0.083740\n0.027809\n0.173816\n-0.039493\n-0.004222\n0.003064\n\n\n\n\n442 rows × 10 columns\n\n\n\n\ny\n\n0      151.0\n1       75.0\n2      141.0\n3      206.0\n4      135.0\n       ...  \n437    178.0\n438    104.0\n439    132.0\n440    220.0\n441     57.0\nName: target, Length: 442, dtype: float64\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\nrfr = RandomForestRegressor().fit(X_train, y_train)\n\n\nrfr.score(X_test, y_test)\n\n0.4145806969881506\n\n\n\ninspector = get_inspector(rfr, X_test, y_test)\n\nYou can then use various methods of inspector to learn about how your model behaves on that data.\nThe methods that are available for a given inspector depends on the types of its estimator and its target y. An attribute called methods tells you what they are:\n\ninspector.methods\n\n['plot_feature_clusters',\n 'plot_partial_dependence',\n 'permutation_importance',\n 'plot_permutation_importance',\n 'plot_pred_vs_act',\n 'plot_residuals',\n 'show_correlation']\n\n\n\nax = inspector.plot_feature_clusters()\n\n\n\n\n\nmost_important_features = inspector.permutation_importance().index[:2]\naxes = inspector.plot_partial_dependence(\n    features=[*most_important_features, most_important_features]\n)\naxes[0, 0].get_figure().set_size_inches(12, 3)\n\n\n\n\n\ninspector.permutation_importance()\n\nbmi    0.241886\ns5     0.153085\nsex    0.003250\ns3     0.000734\nbp     0.000461\ns4    -0.002687\ns2    -0.004366\ns1    -0.008953\ns6    -0.018925\nage   -0.022768\ndtype: float64\n\n\n\nax = inspector.plot_permutation_importance()\n\n\n\n\n\nax = inspector.plot_pred_vs_act()\n\n\n\n\n\naxes = inspector.plot_residuals()\n\n\n\n\n\ninspector.show_correlation()\n\n\n\n\n\n\n \nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\nage\n1.00\n0.22\n0.18\n0.19\n0.23\n0.18\n-0.04\n0.19\n0.28\n0.32\n0.13\n\n\nsex\n0.22\n1.00\n0.29\n0.31\n-0.05\n0.08\n-0.41\n0.30\n0.13\n0.27\n0.27\n\n\nbmi\n0.18\n0.29\n1.00\n0.55\n0.16\n0.18\n-0.43\n0.45\n0.43\n0.49\n0.66\n\n\nbp\n0.19\n0.31\n0.55\n1.00\n0.09\n0.04\n-0.20\n0.19\n0.36\n0.44\n0.51\n\n\ns1\n0.23\n-0.05\n0.16\n0.09\n1.00\n0.88\n0.07\n0.57\n0.50\n0.26\n0.09\n\n\ns2\n0.18\n0.08\n0.18\n0.04\n0.88\n1.00\n-0.16\n0.66\n0.23\n0.18\n0.09\n\n\ns3\n-0.04\n-0.41\n-0.43\n-0.20\n0.07\n-0.16\n1.00\n-0.72\n-0.37\n-0.30\n-0.46\n\n\ns4\n0.19\n0.30\n0.45\n0.19\n0.57\n0.66\n-0.72\n1.00\n0.60\n0.41\n0.41\n\n\ns5\n0.28\n0.13\n0.43\n0.36\n0.50\n0.23\n-0.37\n0.60\n1.00\n0.52\n0.46\n\n\ns6\n0.32\n0.27\n0.49\n0.44\n0.26\n0.18\n-0.30\n0.41\n0.52\n1.00\n0.35\n\n\ntarget\n0.13\n0.27\n0.66\n0.51\n0.09\n0.09\n-0.46\n0.41\n0.46\n0.35\n1.00"
  },
  {
    "objectID": "index.html#scope",
    "href": "index.html#scope",
    "title": "Model Inspector",
    "section": "Scope",
    "text": "Scope\nmodel_inspector makes some attempt to support estimators from popular libraries other than scikit-learn that mimic the scikit-learn interface. The following estimators are specifically supported:\n\nFrom catboost:\n\nCatBoostClassifier\nCatBoostRegressor\n\nFrom lightgbm:\n\nLGBMClassifier\nLGBMRegressor\n\nFrom xgboost:\n\nXGBClassifier\nXGBRegressor"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Model Inspector",
    "section": "Install",
    "text": "Install\npip install model_inspector"
  },
  {
    "objectID": "index.html#alternatives",
    "href": "index.html#alternatives",
    "title": "Model Inspector",
    "section": "Alternatives",
    "text": "Alternatives\n\nYellowbrick\nYellowbrick is similar to Model Inspector in that it provides tools for visualizing the behavior of scikit-learn models.\nThe two libraries have different designs. Yellowbrick uses Visualizer objects, each class of which corresponds to a single type of visualization. The Visualizer interface is similar to the scikit-learn transformer and estimator interfaces. In constrast, model_inspector uses Inspector objects that bundle together a scikit-learn model, an X feature DataFrame, and a y target Series. The Inspector object does the work of identifying appropriate visualization types for the specific model and dataset in question and exposing corresponding methods, making it easy to visualize a given model for a given dataset in a variety of ways.\nAnother fundamental difference is that Yellowbrick is framed as a machine learning visualization library, while Model Inspector treats visualization as just one approach to inspecting the behavior of machine learning models.\n\n\nSHAP\nSHAP is another library that provides a set of tools for understanding the behavior of machine learning models. It has a somewhat similar design to Model Inspector in that it uses Explainer objects to provide access to methods that are appropriate for a given model. It has broader scope than Model Inspector in that it supports models from frameworks such as PyTorch and TensorFlow. It has narrower scope in that it only implements methods based on Shapley values."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Model Inspector",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nMany aspects of this library were inspired by FastAI courses, including bundling together a model with data in a class and providing certain specific visualization methods such as feature importance bar plots, feature clusters dendrograms, tree diagrams, waterfall plots, and partial dependence plots. Its primary contribution is to make all of these methods available in a single convenient interface."
  },
  {
    "objectID": "searchcv_estimator.html",
    "href": "searchcv_estimator.html",
    "title": "SearchCV",
    "section": "",
    "text": "import sklearn.datasets\nfrom model_inspector import get_inspector\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n\n\ngrid = GridSearchCV(\n    estimator=DecisionTreeRegressor(),\n    param_grid={\n        \"min_samples_leaf\": (1, 2, 5, 10, 20, 50),\n        \"max_features\": (0.3, 0.5, 1.0),\n        \"splitter\": (\"best\", \"random\"),\n    },\n    n_jobs=-1,\n    return_train_score=True,\n    cv=KFold(5, shuffle=True),\n).fit(X, y)\n\n\ninspector = get_inspector(grid, X, y)\n\n\nsource\n\n_SearchCVInspector.plot_scores_vs_hparam\n\n _SearchCVInspector.plot_scores_vs_hparam (hparam:Optional[str]=None,\n                                           score_cols:Union[str,Sequence[s\n                                           tr],NoneType]=None, ax=None)\n\nPlot model scores against values of one hyperparameter\nParameters:\n\nhparam: Name of the hyperparameter to plot against. Must be provided if there are multiple hyperparameters. Any other hyperparameters will be fixed at the value they have in self.model.best_params_.\nscore_cols: Name of score columns to plot. By default will be the mean test and (if present) train score for the primary scoring metric.\nax: Matplotlib Axes object. Plot will be added to this object if provided; otherwise a new Axes object will be generated.\n\n\nax = inspector.plot_scores_vs_hparam(\"min_samples_leaf\")\n\n\n\n\n\nax = inspector.plot_scores_vs_hparam(\"max_features\")\n\n\n\n\n\nax = inspector.plot_scores_vs_hparam(\"splitter\")\n\n\n\n\n\nsource\n\n\n_SearchCVInspector.show_score_vs_hparam_pair\n\n _SearchCVInspector.show_score_vs_hparam_pair (hparams=None,\n                                               score_col=None,\n                                               cmap:str|Colormap='PuBu',\n                                               low:float=0, high:float=0,\n                                               axis:Axis|None=0,\n                                               subset:Subset|None=None, te\n                                               xt_color_threshold:float=0.\n                                               408, vmin:float|None=None,\n                                               vmax:float|None=None,\n                                               gmap:Sequence|None=None)\n\nShow model scores against a pair of hyperparameters.\nBackground gradient uses axis=None by default, to facilitate identifying the best score across all combinations of hyperparameter values.\nParameters:\n\nhparams: Name of the hyperparameters to plot against. The first two hyperparameters in self.model.param_grid will be used by default. Any other hyperparameters will be fixed at the value they have in self.model.best_params_.\nscore_col: Name of score column to plot. By default will be the mean test score for the primary scoring metric.\n\nRemaining parameters are passed to pandas.io.formats.style.background_gradient.\n\ninspector.show_score_vs_hparam_pair([\"min_samples_leaf\", \"max_features\"])\n\n\n\n\n\n\nparam_max_features\n0.300000\n0.500000\n1.000000\n\n\nparam_min_samples_leaf\n \n \n \n\n\n\n\n1\n0.702171\n0.679882\n0.696289\n\n\n2\n0.736160\n0.722219\n0.742642\n\n\n5\n0.788082\n0.766575\n0.779670\n\n\n10\n0.754099\n0.768806\n0.771306\n\n\n20\n0.723585\n0.728852\n0.746209\n\n\n50\n0.614502\n0.671836\n0.686244"
  },
  {
    "objectID": "class_utils.html",
    "href": "class_utils.html",
    "title": "Class Utils",
    "section": "",
    "text": "source\n\ndelegates\n\n delegates (to=None, keep=False)\n\nDecorator: replace **kwargs in signature with params from to\nAdapted from https://fastcore.fast.ai/meta.html#delegates"
  },
  {
    "objectID": "classifier.html",
    "href": "classifier.html",
    "title": "Classifier",
    "section": "",
    "text": "import sklearn.datasets\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom model_inspector import get_inspector\n\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\ninspector = get_inspector(\n    RandomForestClassifier().fit(X_train, y_train),\n    X_test,\n    y_test,\n)\n\n/Users/greg/repos/model_inspector/model_inspector/inspect/any_model.py:56: UserWarning: `model` does not have the `feature_names_in_`\n                attribute, so we cannot confirm that `model`'s feature\n                names match `X`'s column names. Proceed at your own\n                risk!\n                \n  warnings.warn(\n\n\n\nsource\n\n\n\n _BinInspector.calculate_metrics_by_thresh\n                                            (metrics:Union[Callable,Sequen\n                                            ce[Callable]], thresholds:Opti\n                                            onal[Sequence]=None)\n\nCalculate classification metrics as a function of threshold\nAssumes that self.model has a .predict_proba() method. Uses self.y as ground-truth values, self.model.predict_proba(self.X)[:, 1] &gt; thresh as predictions.\nParameters:\n\nmetrics: Callables that take y_true, y_pred as positional arguments and return a number. Must have a __name__ attribute.\nthresholds: Sequence of float threshold values to use. By default uses 0 and the values that appear in y_prob[:, 1], which is a minimal set that covers all of the relevant possibilities. One reason to override that default would be to save time with a large dataset.\n\nReturns: DataFrame with one column “thresh” indicating the thresholds used and an additional column for each input metric giving the value of that metric at that threshold.\n\nmetrics_by_thresh = inspector.calculate_metrics_by_thresh(\n    metrics=[metrics.precision_score, metrics.recall_score, metrics.f1_score],\n).iloc[\n    :-1\n]  # dropping last row where precision is undefined\nwith pd.option_context(\"display.max_rows\", 10):\n    display(metrics_by_thresh)  # noqa: F821\n\n 43%|████▎     | 18/42 [00:00&lt;00:00, 178.48it/s]/Users/greg/.pyenv/versions/model_inspector/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n100%|██████████| 42/42 [00:00&lt;00:00, 232.76it/s]\n\n\n\n\n\n\n\n\n\nthresh\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n0\n0.00\n0.713115\n1.000000\n0.832536\n\n\n1\n0.00\n0.713115\n1.000000\n0.832536\n\n\n2\n0.01\n0.737288\n1.000000\n0.848780\n\n\n3\n0.02\n0.756522\n1.000000\n0.861386\n\n\n4\n0.04\n0.783784\n1.000000\n0.878788\n\n\n...\n...\n...\n...\n...\n\n\n36\n0.94\n1.000000\n0.747126\n0.855263\n\n\n37\n0.96\n1.000000\n0.701149\n0.824324\n\n\n38\n0.97\n1.000000\n0.643678\n0.783217\n\n\n39\n0.98\n1.000000\n0.563218\n0.720588\n\n\n40\n0.99\n1.000000\n0.459770\n0.629921\n\n\n\n\n41 rows × 4 columns\n\n\n\n\nax = metrics_by_thresh.plot(x=\"thresh\")\n\n\n\n\n\nsource\n\n\n\n\n _BinInspector.plot_pr_curve\n                              (ax:Optional[matplotlib.axes._axes.Axes]=Non\n                              e, pos_label=None, sample_weight=None)\n\nPlot the precision-recall curve.\nParameters:\n\nax: Matplotlib Axes object. Plot will be added to this object if provided; otherwise a new Axes object will be generated.\n\nRemaining parameters are passed to model_inspector.tune.plot_pr_curve.\n\nax = inspector.plot_pr_curve()\n\n\n\n\n\nsource\n\n\n\n\n _BinInspector.confusion_matrix (thresh:float=0.5, labels=None,\n                                 sample_weight=None, normalize=None)\n\nGet confusion matrix\nAssumes that self.model has a .predict_proba() method. Uses self.y as ground-truth values, self.model.predict_proba(self.X)[:, 1] &gt; thresh as predictions.\nIf output is not rendering properly when you reopen a notebook, make sure the notebook is trusted.\nParameters:\n\nthresh: Probability threshold for counting a prediction as positive\n\nRemaining parameters are passed to sklearn.metrics._classification.confusion_matrix.\n\ninspector.confusion_matrix(\n    thresh=metrics_by_thresh.loc[metrics_by_thresh.f1_score.idxmax(), \"thresh\"]\n)\n\n\n\n\n\n\n \nPredicted 0\nPredicted 1\nTotals\n\n\n\n\nActual 0\n53\n3\n56\n\n\nActual 1\n1\n86\n87\n\n\nTotals\n54\n89\n143"
  },
  {
    "objectID": "classifier.html#binary-classification",
    "href": "classifier.html#binary-classification",
    "title": "Classifier",
    "section": "",
    "text": "import sklearn.datasets\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom model_inspector import get_inspector\n\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\ninspector = get_inspector(\n    RandomForestClassifier().fit(X_train, y_train),\n    X_test,\n    y_test,\n)\n\n/Users/greg/repos/model_inspector/model_inspector/inspect/any_model.py:56: UserWarning: `model` does not have the `feature_names_in_`\n                attribute, so we cannot confirm that `model`'s feature\n                names match `X`'s column names. Proceed at your own\n                risk!\n                \n  warnings.warn(\n\n\n\nsource\n\n\n\n _BinInspector.calculate_metrics_by_thresh\n                                            (metrics:Union[Callable,Sequen\n                                            ce[Callable]], thresholds:Opti\n                                            onal[Sequence]=None)\n\nCalculate classification metrics as a function of threshold\nAssumes that self.model has a .predict_proba() method. Uses self.y as ground-truth values, self.model.predict_proba(self.X)[:, 1] &gt; thresh as predictions.\nParameters:\n\nmetrics: Callables that take y_true, y_pred as positional arguments and return a number. Must have a __name__ attribute.\nthresholds: Sequence of float threshold values to use. By default uses 0 and the values that appear in y_prob[:, 1], which is a minimal set that covers all of the relevant possibilities. One reason to override that default would be to save time with a large dataset.\n\nReturns: DataFrame with one column “thresh” indicating the thresholds used and an additional column for each input metric giving the value of that metric at that threshold.\n\nmetrics_by_thresh = inspector.calculate_metrics_by_thresh(\n    metrics=[metrics.precision_score, metrics.recall_score, metrics.f1_score],\n).iloc[\n    :-1\n]  # dropping last row where precision is undefined\nwith pd.option_context(\"display.max_rows\", 10):\n    display(metrics_by_thresh)  # noqa: F821\n\n 43%|████▎     | 18/42 [00:00&lt;00:00, 178.48it/s]/Users/greg/.pyenv/versions/model_inspector/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n100%|██████████| 42/42 [00:00&lt;00:00, 232.76it/s]\n\n\n\n\n\n\n\n\n\nthresh\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n0\n0.00\n0.713115\n1.000000\n0.832536\n\n\n1\n0.00\n0.713115\n1.000000\n0.832536\n\n\n2\n0.01\n0.737288\n1.000000\n0.848780\n\n\n3\n0.02\n0.756522\n1.000000\n0.861386\n\n\n4\n0.04\n0.783784\n1.000000\n0.878788\n\n\n...\n...\n...\n...\n...\n\n\n36\n0.94\n1.000000\n0.747126\n0.855263\n\n\n37\n0.96\n1.000000\n0.701149\n0.824324\n\n\n38\n0.97\n1.000000\n0.643678\n0.783217\n\n\n39\n0.98\n1.000000\n0.563218\n0.720588\n\n\n40\n0.99\n1.000000\n0.459770\n0.629921\n\n\n\n\n41 rows × 4 columns\n\n\n\n\nax = metrics_by_thresh.plot(x=\"thresh\")\n\n\n\n\n\nsource\n\n\n\n\n _BinInspector.plot_pr_curve\n                              (ax:Optional[matplotlib.axes._axes.Axes]=Non\n                              e, pos_label=None, sample_weight=None)\n\nPlot the precision-recall curve.\nParameters:\n\nax: Matplotlib Axes object. Plot will be added to this object if provided; otherwise a new Axes object will be generated.\n\nRemaining parameters are passed to model_inspector.tune.plot_pr_curve.\n\nax = inspector.plot_pr_curve()\n\n\n\n\n\nsource\n\n\n\n\n _BinInspector.confusion_matrix (thresh:float=0.5, labels=None,\n                                 sample_weight=None, normalize=None)\n\nGet confusion matrix\nAssumes that self.model has a .predict_proba() method. Uses self.y as ground-truth values, self.model.predict_proba(self.X)[:, 1] &gt; thresh as predictions.\nIf output is not rendering properly when you reopen a notebook, make sure the notebook is trusted.\nParameters:\n\nthresh: Probability threshold for counting a prediction as positive\n\nRemaining parameters are passed to sklearn.metrics._classification.confusion_matrix.\n\ninspector.confusion_matrix(\n    thresh=metrics_by_thresh.loc[metrics_by_thresh.f1_score.idxmax(), \"thresh\"]\n)\n\n\n\n\n\n\n \nPredicted 0\nPredicted 1\nTotals\n\n\n\n\nActual 0\n53\n3\n56\n\n\nActual 1\n1\n86\n87\n\n\nTotals\n54\n89\n143"
  },
  {
    "objectID": "classifier.html#multiclass-classification",
    "href": "classifier.html#multiclass-classification",
    "title": "Classifier",
    "section": "Multiclass Classification",
    "text": "Multiclass Classification\n\nfrom functools import partial\n\nfrom model_inspector.tune import calculate_metric_ignoring_nan, coverage\n\n\nprecision_ignoring_nan = partial(\n    calculate_metric_ignoring_nan,\n    metric=partial(metrics.precision_score, average=\"micro\"),\n)\nprecision_ignoring_nan.__name__ = \"precision_ignoring_nan\"\n\nX, y = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\ninspector = get_inspector(\n    RandomForestClassifier().fit(X_train.iloc[:, [3]], y_train),\n    X_test.iloc[:, [3]],\n    y_test,\n)\n\n/Users/greg/repos/model_inspector/model_inspector/inspect/any_model.py:56: UserWarning: `model` does not have the `feature_names_in_`\n                attribute, so we cannot confirm that `model`'s feature\n                names match `X`'s column names. Proceed at your own\n                risk!\n                \n  warnings.warn(\n\n\n\nsource\n\n_MultiInspector.calculate_metrics_by_thresh\n\n _MultiInspector.calculate_metrics_by_thresh\n                                              (metrics:Union[Callable,Sequ\n                                              ence[Callable]], thresholds:\n                                              Optional[Sequence]=None)\n\nCalculate classification metrics as a function of threshold\nAssumes that self.model has a .predict_proba() method. Uses self.y as ground-truth values, uses the value with the highest probability as the prediction if that probability exceeds the threshold, np.nan otherwise.\nParameters:\n\nmetrics: Callables that take y_true, y_pred as positional arguments and return a number. Must have a __name__ attribute and must be able to handle np.nan values.\nthresholds: Sequence of float threshold values to use. By default uses 0 and all values that appear in y_prob, which is a minimal set that covers all of the relevant possibilities. One reason to override that default would be to save time with a large dataset.\n\nReturns: DataFrame with one column “thresh” indicating the thresholds used and an additional column for each input metric giving the value of that metric at that threshold.\n\nmetrics_by_thresh = inspector.calculate_metrics_by_thresh(\n    metrics=[coverage, precision_ignoring_nan],\n).iloc[\n    :-1\n]  # dropping last row where precision is undefined\nmetrics_by_thresh\n\n  0%|          | 0/13 [00:00&lt;?, ?it/s]/Users/greg/.pyenv/versions/model_inspector/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n100%|██████████| 13/13 [00:00&lt;00:00, 2202.71it/s]\n\n\n\n\n\n\n\n\n\nthresh\ncoverage\nprecision_ignoring_nan\n\n\n\n\n0\n0.000000\n1.000000\n0.947368\n\n\n1\n0.000000\n1.000000\n0.947368\n\n\n2\n0.023326\n1.000000\n0.947368\n\n\n3\n0.032452\n1.000000\n0.947368\n\n\n4\n0.107819\n1.000000\n0.947368\n\n\n5\n0.162547\n1.000000\n0.947368\n\n\n6\n0.191166\n1.000000\n0.947368\n\n\n7\n0.808834\n0.947368\n0.944444\n\n\n8\n0.837453\n0.921053\n0.942857\n\n\n9\n0.892181\n0.815789\n0.935484\n\n\n10\n0.967548\n0.789474\n0.966667\n\n\n11\n0.976674\n0.736842\n1.000000\n\n\n\n\n\n\n\n\nax = metrics_by_thresh.iloc[:-1].plot(x=\"thresh\")\n\n\n\n\n\nsource\n\n\n_MultiInspector.confusion_matrix\n\n _MultiInspector.confusion_matrix (cmap:str|Colormap='PuBu', low:float=0,\n                                   high:float=0, axis:Axis|None=0,\n                                   subset:Subset|None=None,\n                                   text_color_threshold:float=0.408,\n                                   vmin:float|None=None,\n                                   vmax:float|None=None,\n                                   gmap:Sequence|None=None)\n\nGet confusion matrix\nUses self.y as ground-truth values, self.model.predict(self.X) as predictions.\nIf output is not rendering properly when you reopen a notebook, make sure the notebook is trusted.\nRemaining parameters are passed to pandas.io.formats.style.background_gradient.\n\ninspector.confusion_matrix()\n\n\n\n\n\n\n \nPredicted 0\nPredicted 1\nPredicted 2\nTotals\n\n\n\n\nActual 0\n16\n0\n0\n16\n\n\nActual 1\n0\n10\n1\n11\n\n\nActual 2\n0\n1\n10\n11\n\n\nTotals\n16\n11\n11\n38"
  },
  {
    "objectID": "regressor.html",
    "href": "regressor.html",
    "title": "Regressor",
    "section": "",
    "text": "import sklearn.datasets\nfrom sklearn.linear_model import LinearRegression\n\nfrom model_inspector import get_inspector\n\n\nX, y = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\ninspector = get_inspector(LinearRegression().fit(X, y), X, y)\n\n\nsource\n\n_RegInspector.plot_residuals\n\n _RegInspector.plot_residuals (axes:Optional[&lt;built-\n                               infunctionarray&gt;]=None,\n                               scatter_kwargs:Optional[dict]=None,\n                               line_kwargs:Optional[dict]=None,\n                               hist_kwargs:Optional[dict]=None)\n\nPlot residuals.\nParameters:\n\naxes: 1D array of two Matplotlib Axes objects. Plot will be added to these objects if provided; otherwise a new array of Axes objects will be generated.\nscatter_kwargs: kwargs to pass to plt.scatter\nline_kwargs: kwargs to pass to plt.plot for line at y=0\nhist_kwargs: kwargs to pass to plt.hist for histogram of residuals\n\n\naxes = inspector.plot_residuals()\n\n\n\n\n\nsource\n\n\n_RegInspector.plot_pred_vs_act\n\n _RegInspector.plot_pred_vs_act\n                                 (ax:Optional[matplotlib.axes._axes.Axes]=\n                                 None, scatter_kwargs:Optional[dict]=None,\n                                 line_kwargs:Optional[dict]=None)\n\nPlot predicted vs. actual values.\nParameters:\n\nax: Matplotlib Axes object. Plot will be added to this object if provided; otherwise a new Axes object will be generated.\nscatter_kwargs: kwargs to pass to plt.scatter\nline_kwargs: kwargs to pass to plt.plot\n\n\nax = inspector.plot_pred_vs_act()"
  },
  {
    "objectID": "tree.html",
    "href": "tree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "import sklearn.datasets\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\nfrom model_inspector import get_inspector\nsource"
  },
  {
    "objectID": "tree.html#regression-example",
    "href": "tree.html#regression-example",
    "title": "Decision Tree",
    "section": "Regression Example",
    "text": "Regression Example\n\nX, y = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n\n\ninspector = get_inspector(DecisionTreeRegressor(max_depth=3).fit(X, y), X, y)\n\n\nax = inspector.plot_tree()"
  },
  {
    "objectID": "tree.html#binary-classification-example",
    "href": "tree.html#binary-classification-example",
    "title": "Decision Tree",
    "section": "Binary Classification Example",
    "text": "Binary Classification Example\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n\n\ninspector = get_inspector(DecisionTreeClassifier(max_depth=3).fit(X, y), X, y)\n\n\nax = inspector.plot_tree()"
  },
  {
    "objectID": "tree.html#multiclass-example",
    "href": "tree.html#multiclass-example",
    "title": "Decision Tree",
    "section": "Multiclass Example",
    "text": "Multiclass Example\n\nX, y = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\n\n\ndtr = DecisionTreeClassifier(max_depth=3).fit(X, y)\ninspector = get_inspector(dtr, X, y)\n\n\nax = inspector.plot_tree()"
  },
  {
    "objectID": "explore.html",
    "href": "explore.html",
    "title": "Explore Data",
    "section": "",
    "text": "source\n\nshow_correlation\n\n show_correlation (df:pandas.core.frame.DataFrame, method='pearson',\n                   cmap:str|Colormap='PuBu', low:float=0, high:float=0,\n                   axis:Axis|None=0, subset:Subset|None=None,\n                   text_color_threshold:float=0.408, vmin:float|None=None,\n                   vmax:float|None=None, gmap:Sequence|None=None)\n\nShow correlation heatmap\nIf output is not rendering properly when you reopen a notebook, make sure the notebook is trusted.\nParameters:\n\ndf: DataFrame\nmethod: Method of correlation to pass to df.corr()\nRemaining parameters are passed to pandas.io.formats.style.background_gradient.\n\n\nimport sklearn.datasets\n\n\nX_diabetes, y_diabetes = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n\n\nshow_correlation(pd.concat((X_diabetes, y_diabetes), axis=\"columns\"))\n\n\n\n\n\n\n \nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\nage\n1.00\n0.17\n0.19\n0.34\n0.26\n0.22\n-0.08\n0.20\n0.27\n0.30\n0.19\n\n\nsex\n0.17\n1.00\n0.09\n0.24\n0.04\n0.14\n-0.38\n0.33\n0.15\n0.21\n0.04\n\n\nbmi\n0.19\n0.09\n1.00\n0.40\n0.25\n0.26\n-0.37\n0.41\n0.45\n0.39\n0.59\n\n\nbp\n0.34\n0.24\n0.40\n1.00\n0.24\n0.19\n-0.18\n0.26\n0.39\n0.39\n0.44\n\n\ns1\n0.26\n0.04\n0.25\n0.24\n1.00\n0.90\n0.05\n0.54\n0.52\n0.33\n0.21\n\n\ns2\n0.22\n0.14\n0.26\n0.19\n0.90\n1.00\n-0.20\n0.66\n0.32\n0.29\n0.17\n\n\ns3\n-0.08\n-0.38\n-0.37\n-0.18\n0.05\n-0.20\n1.00\n-0.74\n-0.40\n-0.27\n-0.39\n\n\ns4\n0.20\n0.33\n0.41\n0.26\n0.54\n0.66\n-0.74\n1.00\n0.62\n0.42\n0.43\n\n\ns5\n0.27\n0.15\n0.45\n0.39\n0.52\n0.32\n-0.40\n0.62\n1.00\n0.46\n0.57\n\n\ns6\n0.30\n0.21\n0.39\n0.39\n0.33\n0.29\n-0.27\n0.42\n0.46\n1.00\n0.38\n\n\ntarget\n0.19\n0.04\n0.59\n0.44\n0.21\n0.17\n-0.39\n0.43\n0.57\n0.38\n1.00\n\n\n\n\n\n\niris = sklearn.datasets.load_iris()\nX_iris, y_iris = iris[\"data\"], iris[\"target\"]\ny_iris = pd.Series(y_iris, name=\"iris type\").map(\n    {num: name for num, name in zip([0, 1, 2], iris[\"target_names\"])}\n)\nX_iris = pd.DataFrame(X_iris, columns=iris[\"feature_names\"])\n\n\nshow_correlation(\n    pd.concat((X_iris, pd.Series(y_iris == \"setosa\", name=\"setosa\"))), method=\"spearman\"\n)\n\n/var/folders/wv/pmfhhk1d4h1fkd_z5l83m0dw0000gq/T/ipykernel_96080/529445965.py:15: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  return df.corr(method=method).style.background_gradient(**kwargs).format(\"{0:,.2f}\")\n\n\n\n\n\n\n\n \nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\nsepal length (cm)\n1.00\n-0.17\n0.88\n0.83\n\n\nsepal width (cm)\n-0.17\n1.00\n-0.31\n-0.29\n\n\npetal length (cm)\n0.88\n-0.31\n1.00\n0.94\n\n\npetal width (cm)\n0.83\n-0.29\n0.94\n1.00\n\n\n\n\n\n\nsource\n\n\nplot_column_clusters\n\n plot_column_clusters (df, corr_method:str='spearman',\n                       ax:matplotlib.axes._axes.Axes=None, p=30,\n                       truncate_mode=None, color_threshold=None,\n                       get_leaves=True, orientation='top', labels=None,\n                       count_sort=False, distance_sort=False,\n                       show_leaf_counts=True, no_plot=False,\n                       no_labels=False, leaf_font_size=None,\n                       leaf_rotation=None, leaf_label_func=None,\n                       show_contracted=False, link_color_func=None,\n                       above_threshold_color='C0')\n\nPlot a dendrogram based on column correlations\nIf output is not rendering properly when you reopen a notebook, make sure the notebook is trusted.\nAdapted from https://github.com/fastai/book_nbs/blob/master/utils.py#L58-L64\nParameters:\n\ndf: DataFrame\ncorr_method: Method of correlation to pass to df.corr()\nax: Matplotlib Axes object. Plot will be added to this object if provided; otherwise a new Axes object will be generated.\nRemaining parameters are passed to scipy.cluster.hierarchy.dendrogram.\n\n\nax = plot_column_clusters(X_iris)"
  },
  {
    "objectID": "tune.html",
    "href": "tune.html",
    "title": "Tune a Model",
    "section": "",
    "text": "source\n\ncalculate_metrics_by_thresh_binary\n\n calculate_metrics_by_thresh_binary (y_true:&lt;built-infunctionarray&gt;,\n                                     y_prob:&lt;built-infunctionarray&gt;, metri\n                                     cs:Union[Callable,Sequence[Callable]]\n                                     , thresholds:Optional[Sequence]=None)\n\nCalculate binary classification metrics as a function of threshold\nTakes prediction to be 1 when y_prob is greater than the threshold, 0 otherwise.\nParameters:\n\ny_true: Ground-truth values with shape (n_items,)\ny_prob: Probability distributions with shape (n_items, 2)\nmetrics: Callables that take y_true, y_pred as positional arguments and return a number. Must have a __name__ attribute.\nthresholds: Sequence of float threshold values to use. By default uses 0 and the values that appear in y_prob[:, 1], which is a minimal set that covers all of the relevant possibilities. One reason to override that default would be to save time with a large dataset.\n\nReturns: DataFrame with one column “thresh” indicating the thresholds used and an additional column for each input metric giving the value of that metric at that threshold.\nFor instance, we can use calculate_metrics_by_thresh_binary to find the threshold that maximizes a model’s F1 score.\n\ny_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\ny_prob = np.array(\n    [\n        [0.9, 0.1],\n        [0.7, 0.3],\n        [0.6, 0.4],\n        [0.6, 0.4],\n        [0.4, 0.6],\n        [0.6, 0.4],\n        [0.4, 0.6],\n        [0.4, 0.6],\n        [0.3, 0.7],\n        [0.1, 0.9],\n    ]\n)\n\nresults = calculate_metrics_by_thresh_binary(\n    y_true=y_true,\n    y_prob=y_prob,\n    metrics=[metrics.recall_score, metrics.precision_score, metrics.f1_score],\n).iloc[:-1, :]\nresults\n\n  0%|          | 0/7 [00:00&lt;?, ?it/s]/Users/greg/.pyenv/versions/model_inspector/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n100%|██████████| 7/7 [00:00&lt;00:00, 472.74it/s]\n\n\n\n\n\n\n\n\n\nthresh\nrecall_score\nprecision_score\nf1_score\n\n\n\n\n0\n0.0\n1.0\n0.500000\n0.666667\n\n\n1\n0.1\n1.0\n0.555556\n0.714286\n\n\n2\n0.3\n1.0\n0.625000\n0.769231\n\n\n3\n0.4\n0.8\n0.800000\n0.800000\n\n\n4\n0.6\n0.4\n1.000000\n0.571429\n\n\n5\n0.7\n0.2\n1.000000\n0.333333\n\n\n\n\n\n\n\n\nax = results.plot(x=\"thresh\")\nbest_row = results.loc[results.loc[:, \"f1_score\"].idxmax()]\nbest_thresh = best_row.loc[\"thresh\"]\nax.axvline(best_thresh, c=\"k\")\n\nprint(\"Best result:\")\nbest_row\n\nBest result:\n\n\nthresh             0.4\nrecall_score       0.8\nprecision_score    0.8\nf1_score           0.8\nName: 3, dtype: float64\n\n\n\n\n\n\nsource\n\n\nplot_pr_curve\n\n plot_pr_curve (y_true:&lt;built-infunctionarray&gt;, y_prob:&lt;built-\n                infunctionarray&gt;,\n                ax:Optional[matplotlib.axes._axes.Axes]=None,\n                pos_label=None, sample_weight=None)\n\nPlot the precision-recall curve for a binary classification problem.\nParameters:\n\ny_true: Ground-truth values with shape (n_items,)\ny_prob: Probability distributions with shape (n_items, 2)\nax: Matplotlib Axes object. Plot will be added to this object if provided; otherwise a new Axes object will be generated.\nRemaining parameters are passed to sklearn.metrics._ranking.precision_recall_curve.\n\n\nax = plot_pr_curve(\n    y_true=y_true,\n    y_prob=y_prob,\n)\n\n\n\n\n\nsource\n\n\ncalculate_metrics_by_thresh_multi\n\n calculate_metrics_by_thresh_multi (y_true:&lt;built-infunctionarray&gt;,\n                                    y_prob:&lt;built-infunctionarray&gt;, metric\n                                    s:Union[Callable,Sequence[Callable]],\n                                    thresholds:Optional[Sequence]=None)\n\nCalculate multiclass metrics as a function of threshold\nTakes prediction to be the position of the column in y_prob with the greatest value if that value is greater than the threshold, np.nan otherwise.\nParameters:\n\ny_true: Ground-truth values\ny_prob: Probability distributions\nmetrics: Callables that take y_true, y_pred as positional arguments and return a number. Must have a __name__ attribute.\nthresholds: Sequence of float threshold values to use. By default uses 0 and all values that appear in y_prob, which is a minimal set that covers all of the relevant possibilities. One reason to override that default would be to save time with a large dataset.\n\nReturns: DataFrame with one column “thresh” indicating the thresholds used and an additional column for each input metric giving the value of that metric at that threshold.\nSuppose that in a multiclass problem we want to track two metrics: coverage (how often we make a prediction) and precision (how often our predictions are right when we make them). We will choose the threshold an \\(F_\\beta\\)-like metric that maximizes a weighted harmonic mean of those two metrics that puts twice as much weight on precision as coverage.\n\nsource\n\n\ncoverage\n\n coverage (y_true:&lt;built-infunctionarray&gt;, y_pred:&lt;built-infunctionarray&gt;)\n\nHow often the model makes a prediction, where np.nan indicates abstaining from predicting.\nParameters:\n\ny_true: Ground-truth values\ny_pred: Predicted values, possibly including np.nan to indicate abstraining from predicting\n\n\nsource\n\n\ncalculate_metric_ignoring_nan\n\n calculate_metric_ignoring_nan (y_true:&lt;built-infunctionarray&gt;,\n                                y_pred:&lt;built-infunctionarray&gt;,\n                                metric:Callable, *args, **kwargs)\n\nCalculate metric ignoring np.nan predictions\nParameters:\n\ny_true: Ground-truth values\ny_pred: Predicted values, possibly including np.nan to indicate abstraining from predicting\nmetric: Function that takes y_true, y_pred as keyword arguments\n\nAny additional arguments will be passed to metric\n\nsource\n\n\nfbeta\n\n fbeta (precision:float, recall:float, beta:float=1)\n\n\nprecision_ignoring_nan = partial(\n    calculate_metric_ignoring_nan,\n    metric=partial(metrics.precision_score, average=\"micro\"),\n)\nprecision_ignoring_nan.__name__ = \"precision_ignoring_nan\"\n\ny_true = np.array([0, 0, 1, 2])\ny_prob = np.array([[0.9, 0.1, 0], [0.2, 0.8, 0], [0.2, 0.8, 0], [0.3, 0.4, 0.3]])\n\nresults = calculate_metrics_by_thresh_multi(\n    y_true=y_true,\n    y_prob=y_prob,\n    metrics=[coverage, precision_ignoring_nan],\n).iloc[:-1, :]\nresults.loc[:, \"quasi_fbeta\"] = results.apply(\n    lambda row: fbeta(\n        precision=row.loc[\"precision_ignoring_nan\"],\n        recall=row.loc[\"coverage\"],\n        beta=0.5,\n    ),\n    axis=\"columns\",\n)\nresults\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/Users/greg/.pyenv/versions/model_inspector/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n100%|██████████| 8/8 [00:00&lt;00:00, 582.12it/s]\n\n\n\n\n\n\n\n\n\nthresh\ncoverage\nprecision_ignoring_nan\nquasi_fbeta\n\n\n\n\n0\n0.0\n1.00\n0.500000\n0.555556\n\n\n1\n0.0\n1.00\n0.500000\n0.555556\n\n\n2\n0.1\n1.00\n0.500000\n0.555556\n\n\n3\n0.2\n1.00\n0.500000\n0.555556\n\n\n4\n0.3\n1.00\n0.500000\n0.555556\n\n\n5\n0.4\n0.75\n0.666667\n0.681818\n\n\n6\n0.8\n0.25\n1.000000\n0.625000\n\n\n\n\n\n\n\n\nax = results.plot(x=\"thresh\")\nbest_row = results.loc[results.loc[:, \"quasi_fbeta\"].idxmax(), :]\nax.axvline(best_row.loc[\"thresh\"], c=\"k\")\nprint(\"Best result:\")\nbest_row\n\nBest result:\n\n\nthresh                    0.400000\ncoverage                  0.750000\nprecision_ignoring_nan    0.666667\nquasi_fbeta               0.681818\nName: 5, dtype: float64\n\n\n\n\n\n\nsource\n\n\nconfusion_matrix\n\n confusion_matrix (y_true:Union[&lt;built-\n                   infunctionarray&gt;,pandas.core.series.Series],\n                   y_pred:Union[&lt;built-\n                   infunctionarray&gt;,pandas.core.series.Series],\n                   shade_axis:Union[str,int,NoneType]=None,\n                   sample_weight:Optional[&lt;built-infunctionarray&gt;]=None,\n                   normalize:Optional[str]=None)\n\nGet confusion matrix\nParameters:\n\ny_true: Ground-truth values\ny_pred: Predicted values\nshade_axis: axis argument to pass to pd.DataFrame.style.background_gradient\n\nThe remaining parameters are passed to sklearn.metrics.confusion_matrix.\n\nconfusion_matrix(\n    y_true=np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]),\n    y_pred=np.array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1]),\n)\n\n\n\n\n\n\n \nPredicted 0\nPredicted 1\nTotals\n\n\n\n\nActual 0\n4\n1\n5\n\n\nActual 1\n2\n3\n5\n\n\nTotals\n6\n4\n10\n\n\n\n\n\n\nconfusion_matrix(\n    y_true=np.array([0, 0, 2, 0, 0, 1, 1, 1, 1, 1]),\n    y_pred=np.array([0, 0, 2, 0, 1, 2, 1, 1, 0, 1]),\n    shade_axis=\"rows\",\n)\n\n\n\n\n\n\n \nPredicted 0\nPredicted 1\nPredicted 2\nTotals\n\n\n\n\nActual 0\n3\n1\n0\n4\n\n\nActual 1\n1\n3\n1\n5\n\n\nActual 2\n0\n0\n1\n1\n\n\nTotals\n4\n4\n2\n10"
  },
  {
    "objectID": "get_inspector.html",
    "href": "get_inspector.html",
    "title": "Get an Inspector",
    "section": "",
    "text": "source\n\nget_inspector\n\n get_inspector (model:sklearn.base.BaseEstimator,\n                X:pandas.core.frame.DataFrame,\n                y:pandas.core.series.Series)\n\nGet an appropriate inspector for your model and data.\nParameters:\n\nmodel: Fitted sklearn model\nX: Matrix with the same features model was trained on\ny: Series with same length as X and same meaning as target values model was trained on\n\nExample:\n\nimport sklearn.datasets\nfrom sklearn.tree import DecisionTreeRegressor\nfrom model_inspector import get_inspector\n\n\nX, y = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n\n\ndtr = DecisionTreeRegressor().fit(X, y)\n\n\ninspector = get_inspector(dtr, X=X, y=y)\ninspector\n\nmodel_inspector.inspect.tree._TreeRegInspector(model=DecisionTreeRegressor())\n\n\n\ninspector.methods\n\n['permutation_importance',\n 'plot_feature_clusters',\n 'plot_partial_dependence',\n 'plot_permutation_importance',\n 'plot_pred_vs_act',\n 'plot_residuals',\n 'plot_tree',\n 'show_correlation']\n\n\n\nax = inspector.plot_tree(max_depth=2)"
  },
  {
    "objectID": "gbm.html",
    "href": "gbm.html",
    "title": "GBMs",
    "section": "",
    "text": "At this time we do not provide functionality specific to gradient boosted machines, but we want to support scikit-learn-like interfaces in popular libraries, including catboost, lightgbm, and xgboost.\n# LightGBM and XGBoost are not Model Inspector requirements. CatBoost is\n# because it requires a small shim.\n! pip install lightgbm xgboost\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom xgboost import XGBClassifier, XGBRegressor\n\nfrom model_inspector import get_inspector"
  },
  {
    "objectID": "gbm.html#catboost",
    "href": "gbm.html#catboost",
    "title": "GBMs",
    "section": "CatBoost",
    "text": "CatBoost\n\nCatBoostRegressor\n\nX, y = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n\n\ninspector = get_inspector(CatBoostRegressor(n_estimators=5).fit(X, y), X, y)\n\nLearning rate set to 0.5\n0:  learn: 63.4011144   total: 896us    remaining: 3.59ms\n1:  learn: 56.1323417   total: 1.42ms   remaining: 2.14ms\n2:  learn: 52.9311271   total: 1.88ms   remaining: 1.25ms\n3:  learn: 50.5129302   total: 2.39ms   remaining: 596us\n4:  learn: 49.1286322   total: 2.77ms   remaining: 0us\n\n\n\ninspector.methods\n\n['permutation_importance',\n 'plot_feature_clusters',\n 'plot_partial_dependence',\n 'plot_permutation_importance',\n 'plot_pred_vs_act',\n 'plot_residuals',\n 'show_correlation']\n\n\n\ninspector.permutation_importance()\n\ns5     0.253769\nbmi    0.250723\nbp     0.130111\nsex    0.059715\ns3     0.051180\ns2     0.038155\ns1     0.020064\ns4     0.018919\ns6     0.015744\nage    0.015219\ndtype: float64\n\n\n\nax = inspector.plot_feature_clusters()\n\n\n\n\n\naxes = inspector.plot_partial_dependence(features=[\"bmi\"])\n\n\n\n\n\nax = inspector.plot_pred_vs_act()\n\n\n\n\n\nax = inspector.plot_residuals()\n\n\n\n\n\ninspector.show_correlation()\n\n\n\n\n\n\n \nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\nage\n1.00\n0.17\n0.19\n0.34\n0.26\n0.22\n-0.08\n0.20\n0.27\n0.30\n0.19\n\n\nsex\n0.17\n1.00\n0.09\n0.24\n0.04\n0.14\n-0.38\n0.33\n0.15\n0.21\n0.04\n\n\nbmi\n0.19\n0.09\n1.00\n0.40\n0.25\n0.26\n-0.37\n0.41\n0.45\n0.39\n0.59\n\n\nbp\n0.34\n0.24\n0.40\n1.00\n0.24\n0.19\n-0.18\n0.26\n0.39\n0.39\n0.44\n\n\ns1\n0.26\n0.04\n0.25\n0.24\n1.00\n0.90\n0.05\n0.54\n0.52\n0.33\n0.21\n\n\ns2\n0.22\n0.14\n0.26\n0.19\n0.90\n1.00\n-0.20\n0.66\n0.32\n0.29\n0.17\n\n\ns3\n-0.08\n-0.38\n-0.37\n-0.18\n0.05\n-0.20\n1.00\n-0.74\n-0.40\n-0.27\n-0.39\n\n\ns4\n0.20\n0.33\n0.41\n0.26\n0.54\n0.66\n-0.74\n1.00\n0.62\n0.42\n0.43\n\n\ns5\n0.27\n0.15\n0.45\n0.39\n0.52\n0.32\n-0.40\n0.62\n1.00\n0.46\n0.57\n\n\ns6\n0.30\n0.21\n0.39\n0.39\n0.33\n0.29\n-0.27\n0.42\n0.46\n1.00\n0.38\n\n\ntarget\n0.19\n0.04\n0.59\n0.44\n0.21\n0.17\n-0.39\n0.43\n0.57\n0.38\n1.00\n\n\n\n\n\n\n\nCatBoostClassifier\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n\n\ninspector = get_inspector(CatBoostClassifier(n_estimators=5).fit(X, y), X, y)\n\nLearning rate set to 0.5\n0:  learn: 0.2068913    total: 2.79ms   remaining: 11.2ms\n1:  learn: 0.1073211    total: 4.88ms   remaining: 7.32ms\n2:  learn: 0.0762370    total: 6.91ms   remaining: 4.61ms\n3:  learn: 0.0572005    total: 8.96ms   remaining: 2.24ms\n4:  learn: 0.0464524    total: 15.5ms   remaining: 0us\n\n\n\ninspector.methods\n\n['calculate_metrics_by_thresh',\n 'confusion_matrix',\n 'permutation_importance',\n 'plot_feature_clusters',\n 'plot_partial_dependence',\n 'plot_permutation_importance',\n 'show_correlation']\n\n\n\nax = inspector.calculate_metrics_by_thresh(\n    sklearn.metrics.accuracy_score, np.linspace(0, 1, 11)\n).plot(x=\"thresh\")\n\n100%|██████████| 11/11 [00:00&lt;00:00, 2964.93it/s]\n\n\n\n\n\n\ninspector.confusion_matrix()\n\n\n\n\n\n\n \nPredicted 0\nPredicted 1\nTotals\n\n\n\n\nActual 0\n208\n4\n212\n\n\nActual 1\n0\n357\n357\n\n\nTotals\n208\n361\n569\n\n\n\n\n\n\ninspector.permutation_importance()\n\nworst area                 0.069947\nmean texture               0.024605\nworst smoothness           0.016872\nworst concavity            0.014411\nworst concave points       0.009490\nmean radius                0.008436\nmean smoothness            0.004921\nmean area                  0.003866\nmean concavity             0.003515\nperimeter error            0.003163\nmean concave points        0.003163\narea error                 0.002812\ncompactness error          0.002812\nworst fractal dimension    0.002109\nworst perimeter            0.002109\nworst texture              0.001757\nmean perimeter             0.001054\nworst compactness          0.000351\nconcavity error            0.000000\nconcave points error       0.000000\nsymmetry error             0.000000\nfractal dimension error    0.000000\nworst radius               0.000000\nsmoothness error           0.000000\nradius error               0.000000\nmean fractal dimension     0.000000\nmean symmetry              0.000000\nmean compactness           0.000000\nworst symmetry             0.000000\ntexture error              0.000000\ndtype: float64\n\n\n\nax = inspector.plot_feature_clusters()\n\n\n\n\n\naxes = inspector.plot_partial_dependence(features=[\"worst texture\"])\n\n\n\n\n\nax = inspector.plot_permutation_importance()\n\n\n\n\n\ninspector.show_correlation()\n\n\n\n\n\n\n \nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\nradius error\ntexture error\nperimeter error\narea error\nsmoothness error\ncompactness error\nconcavity error\nconcave points error\nsymmetry error\nfractal dimension error\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\nmean radius\n1.00\n0.32\n1.00\n0.99\n0.17\n0.51\n0.68\n0.82\n0.15\n-0.31\n0.68\n-0.10\n0.67\n0.74\n-0.22\n0.21\n0.19\n0.38\n-0.10\n-0.04\n0.97\n0.30\n0.97\n0.94\n0.12\n0.41\n0.53\n0.74\n0.16\n0.01\n-0.73\n\n\nmean texture\n0.32\n1.00\n0.33\n0.32\n-0.02\n0.24\n0.30\n0.29\n0.07\n-0.08\n0.28\n0.39\n0.28\n0.26\n0.01\n0.19\n0.14\n0.16\n0.01\n0.05\n0.35\n0.91\n0.36\n0.34\n0.08\n0.28\n0.30\n0.30\n0.11\n0.12\n-0.42\n\n\nmean perimeter\n1.00\n0.33\n1.00\n0.99\n0.21\n0.56\n0.72\n0.85\n0.18\n-0.26\n0.69\n-0.09\n0.69\n0.74\n-0.20\n0.25\n0.23\n0.41\n-0.08\n-0.01\n0.97\n0.30\n0.97\n0.94\n0.15\n0.46\n0.56\n0.77\n0.19\n0.05\n-0.74\n\n\nmean area\n0.99\n0.32\n0.99\n1.00\n0.18\n0.50\n0.69\n0.82\n0.15\n-0.28\n0.73\n-0.07\n0.73\n0.80\n-0.17\n0.21\n0.21\n0.37\n-0.07\n-0.02\n0.96\n0.29\n0.96\n0.96\n0.12\n0.39\n0.51\n0.72\n0.14\n0.00\n-0.71\n\n\nmean smoothness\n0.17\n-0.02\n0.21\n0.18\n1.00\n0.66\n0.52\n0.55\n0.56\n0.58\n0.30\n0.07\n0.30\n0.25\n0.33\n0.32\n0.25\n0.38\n0.20\n0.28\n0.21\n0.04\n0.24\n0.21\n0.81\n0.47\n0.43\n0.50\n0.39\n0.50\n-0.36\n\n\nmean compactness\n0.51\n0.24\n0.56\n0.50\n0.66\n1.00\n0.88\n0.83\n0.60\n0.57\n0.50\n0.05\n0.55\n0.46\n0.14\n0.74\n0.57\n0.64\n0.23\n0.51\n0.54\n0.25\n0.59\n0.51\n0.57\n0.87\n0.82\n0.82\n0.51\n0.69\n-0.60\n\n\nmean concavity\n0.68\n0.30\n0.72\n0.69\n0.52\n0.88\n1.00\n0.92\n0.50\n0.34\n0.63\n0.08\n0.66\n0.62\n0.10\n0.67\n0.69\n0.68\n0.18\n0.45\n0.69\n0.30\n0.73\n0.68\n0.45\n0.75\n0.88\n0.86\n0.41\n0.51\n-0.70\n\n\nmean concave points\n0.82\n0.29\n0.85\n0.82\n0.55\n0.83\n0.92\n1.00\n0.46\n0.17\n0.70\n0.02\n0.71\n0.69\n0.03\n0.49\n0.44\n0.62\n0.10\n0.26\n0.83\n0.29\n0.86\n0.81\n0.45\n0.67\n0.75\n0.91\n0.38\n0.37\n-0.78\n\n\nmean symmetry\n0.15\n0.07\n0.18\n0.15\n0.56\n0.60\n0.50\n0.46\n1.00\n0.48\n0.30\n0.13\n0.31\n0.22\n0.19\n0.42\n0.34\n0.39\n0.45\n0.33\n0.19\n0.09\n0.22\n0.18\n0.43\n0.47\n0.43\n0.43\n0.70\n0.44\n-0.33\n\n\nmean fractal dimension\n-0.31\n-0.08\n-0.26\n-0.28\n0.58\n0.57\n0.34\n0.17\n0.48\n1.00\n0.00\n0.16\n0.04\n-0.09\n0.40\n0.56\n0.45\n0.34\n0.35\n0.69\n-0.25\n-0.05\n-0.21\n-0.23\n0.50\n0.46\n0.35\n0.18\n0.33\n0.77\n0.01\n\n\nradius error\n0.68\n0.28\n0.69\n0.73\n0.30\n0.50\n0.63\n0.70\n0.30\n0.00\n1.00\n0.21\n0.97\n0.95\n0.16\n0.36\n0.33\n0.51\n0.24\n0.23\n0.72\n0.19\n0.72\n0.75\n0.14\n0.29\n0.38\n0.53\n0.09\n0.05\n-0.57\n\n\ntexture error\n-0.10\n0.39\n-0.09\n-0.07\n0.07\n0.05\n0.08\n0.02\n0.13\n0.16\n0.21\n1.00\n0.22\n0.11\n0.40\n0.23\n0.19\n0.23\n0.41\n0.28\n-0.11\n0.41\n-0.10\n-0.08\n-0.07\n-0.09\n-0.07\n-0.12\n-0.13\n-0.05\n0.01\n\n\nperimeter error\n0.67\n0.28\n0.69\n0.73\n0.30\n0.55\n0.66\n0.71\n0.31\n0.04\n0.97\n0.22\n1.00\n0.94\n0.15\n0.42\n0.36\n0.56\n0.27\n0.24\n0.70\n0.20\n0.72\n0.73\n0.13\n0.34\n0.42\n0.55\n0.11\n0.09\n-0.56\n\n\narea error\n0.74\n0.26\n0.74\n0.80\n0.25\n0.46\n0.62\n0.69\n0.22\n-0.09\n0.95\n0.11\n0.94\n1.00\n0.08\n0.28\n0.27\n0.42\n0.13\n0.13\n0.76\n0.20\n0.76\n0.81\n0.13\n0.28\n0.39\n0.54\n0.07\n0.02\n-0.55\n\n\nsmoothness error\n-0.22\n0.01\n-0.20\n-0.17\n0.33\n0.14\n0.10\n0.03\n0.19\n0.40\n0.16\n0.40\n0.15\n0.08\n1.00\n0.34\n0.27\n0.33\n0.41\n0.43\n-0.23\n-0.07\n-0.22\n-0.18\n0.31\n-0.06\n-0.06\n-0.10\n-0.11\n0.10\n0.07\n\n\ncompactness error\n0.21\n0.19\n0.25\n0.21\n0.32\n0.74\n0.67\n0.49\n0.42\n0.56\n0.36\n0.23\n0.42\n0.28\n0.34\n1.00\n0.80\n0.74\n0.39\n0.80\n0.20\n0.14\n0.26\n0.20\n0.23\n0.68\n0.64\n0.48\n0.28\n0.59\n-0.29\n\n\nconcavity error\n0.19\n0.14\n0.23\n0.21\n0.25\n0.57\n0.69\n0.44\n0.34\n0.45\n0.33\n0.19\n0.36\n0.27\n0.27\n0.80\n1.00\n0.77\n0.31\n0.73\n0.19\n0.10\n0.23\n0.19\n0.17\n0.48\n0.66\n0.44\n0.20\n0.44\n-0.25\n\n\nconcave points error\n0.38\n0.16\n0.41\n0.37\n0.38\n0.64\n0.68\n0.62\n0.39\n0.34\n0.51\n0.23\n0.56\n0.42\n0.33\n0.74\n0.77\n1.00\n0.31\n0.61\n0.36\n0.09\n0.39\n0.34\n0.22\n0.45\n0.55\n0.60\n0.14\n0.31\n-0.41\n\n\nsymmetry error\n-0.10\n0.01\n-0.08\n-0.07\n0.20\n0.23\n0.18\n0.10\n0.45\n0.35\n0.24\n0.41\n0.27\n0.13\n0.41\n0.39\n0.31\n0.31\n1.00\n0.37\n-0.13\n-0.08\n-0.10\n-0.11\n-0.01\n0.06\n0.04\n-0.03\n0.39\n0.08\n0.01\n\n\nfractal dimension error\n-0.04\n0.05\n-0.01\n-0.02\n0.28\n0.51\n0.45\n0.26\n0.33\n0.69\n0.23\n0.28\n0.24\n0.13\n0.43\n0.80\n0.73\n0.61\n0.37\n1.00\n-0.04\n-0.00\n-0.00\n-0.02\n0.17\n0.39\n0.38\n0.22\n0.11\n0.59\n-0.08\n\n\nworst radius\n0.97\n0.35\n0.97\n0.96\n0.21\n0.54\n0.69\n0.83\n0.19\n-0.25\n0.72\n-0.11\n0.70\n0.76\n-0.23\n0.20\n0.19\n0.36\n-0.13\n-0.04\n1.00\n0.36\n0.99\n0.98\n0.22\n0.48\n0.57\n0.79\n0.24\n0.09\n-0.78\n\n\nworst texture\n0.30\n0.91\n0.30\n0.29\n0.04\n0.25\n0.30\n0.29\n0.09\n-0.05\n0.19\n0.41\n0.20\n0.20\n-0.07\n0.14\n0.10\n0.09\n-0.08\n-0.00\n0.36\n1.00\n0.37\n0.35\n0.23\n0.36\n0.37\n0.36\n0.23\n0.22\n-0.46\n\n\nworst perimeter\n0.97\n0.36\n0.97\n0.96\n0.24\n0.59\n0.73\n0.86\n0.22\n-0.21\n0.72\n-0.10\n0.72\n0.76\n-0.22\n0.26\n0.23\n0.39\n-0.10\n-0.00\n0.99\n0.37\n1.00\n0.98\n0.24\n0.53\n0.62\n0.82\n0.27\n0.14\n-0.78\n\n\nworst area\n0.94\n0.34\n0.94\n0.96\n0.21\n0.51\n0.68\n0.81\n0.18\n-0.23\n0.75\n-0.08\n0.73\n0.81\n-0.18\n0.20\n0.19\n0.34\n-0.11\n-0.02\n0.98\n0.35\n0.98\n1.00\n0.21\n0.44\n0.54\n0.75\n0.21\n0.08\n-0.73\n\n\nworst smoothness\n0.12\n0.08\n0.15\n0.12\n0.81\n0.57\n0.45\n0.45\n0.43\n0.50\n0.14\n-0.07\n0.13\n0.13\n0.31\n0.23\n0.17\n0.22\n-0.01\n0.17\n0.22\n0.23\n0.24\n0.21\n1.00\n0.57\n0.52\n0.55\n0.49\n0.62\n-0.42\n\n\nworst compactness\n0.41\n0.28\n0.46\n0.39\n0.47\n0.87\n0.75\n0.67\n0.47\n0.46\n0.29\n-0.09\n0.34\n0.28\n-0.06\n0.68\n0.48\n0.45\n0.06\n0.39\n0.48\n0.36\n0.53\n0.44\n0.57\n1.00\n0.89\n0.80\n0.61\n0.81\n-0.59\n\n\nworst concavity\n0.53\n0.30\n0.56\n0.51\n0.43\n0.82\n0.88\n0.75\n0.43\n0.35\n0.38\n-0.07\n0.42\n0.39\n-0.06\n0.64\n0.66\n0.55\n0.04\n0.38\n0.57\n0.37\n0.62\n0.54\n0.52\n0.89\n1.00\n0.86\n0.53\n0.69\n-0.66\n\n\nworst concave points\n0.74\n0.30\n0.77\n0.72\n0.50\n0.82\n0.86\n0.91\n0.43\n0.18\n0.53\n-0.12\n0.55\n0.54\n-0.10\n0.48\n0.44\n0.60\n-0.03\n0.22\n0.79\n0.36\n0.82\n0.75\n0.55\n0.80\n0.86\n1.00\n0.50\n0.51\n-0.79\n\n\nworst symmetry\n0.16\n0.11\n0.19\n0.14\n0.39\n0.51\n0.41\n0.38\n0.70\n0.33\n0.09\n-0.13\n0.11\n0.07\n-0.11\n0.28\n0.20\n0.14\n0.39\n0.11\n0.24\n0.23\n0.27\n0.21\n0.49\n0.61\n0.53\n0.50\n1.00\n0.54\n-0.42\n\n\nworst fractal dimension\n0.01\n0.12\n0.05\n0.00\n0.50\n0.69\n0.51\n0.37\n0.44\n0.77\n0.05\n-0.05\n0.09\n0.02\n0.10\n0.59\n0.44\n0.31\n0.08\n0.59\n0.09\n0.22\n0.14\n0.08\n0.62\n0.81\n0.69\n0.51\n0.54\n1.00\n-0.32\n\n\ntarget\n-0.73\n-0.42\n-0.74\n-0.71\n-0.36\n-0.60\n-0.70\n-0.78\n-0.33\n0.01\n-0.57\n0.01\n-0.56\n-0.55\n0.07\n-0.29\n-0.25\n-0.41\n0.01\n-0.08\n-0.78\n-0.46\n-0.78\n-0.73\n-0.42\n-0.59\n-0.66\n-0.79\n-0.42\n-0.32\n1.00"
  },
  {
    "objectID": "gbm.html#lightgbm",
    "href": "gbm.html#lightgbm",
    "title": "GBMs",
    "section": "LightGBM",
    "text": "LightGBM\n\nLGMBRegressor\n\nX, y = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n\n\ninspector = get_inspector(LGBMRegressor(n_estimators=5).fit(X, y), X, y)\n\nLearning rate set to 0.5\n0:  learn: 63.4011144   total: 896us    remaining: 3.59ms\n1:  learn: 56.1323417   total: 1.42ms   remaining: 2.14ms\n2:  learn: 52.9311271   total: 1.88ms   remaining: 1.25ms\n3:  learn: 50.5129302   total: 2.39ms   remaining: 596us\n4:  learn: 49.1286322   total: 2.77ms   remaining: 0us\n\n\n\ninspector.methods\n\n['permutation_importance',\n 'plot_feature_clusters',\n 'plot_partial_dependence',\n 'plot_permutation_importance',\n 'plot_pred_vs_act',\n 'plot_residuals',\n 'show_correlation']\n\n\n\ninspector.permutation_importance()\n\ns5     0.253769\nbmi    0.250723\nbp     0.130111\nsex    0.059715\ns3     0.051180\ns2     0.038155\ns1     0.020064\ns4     0.018919\ns6     0.015744\nage    0.015219\ndtype: float64\n\n\n\nax = inspector.plot_feature_clusters()\n\n\n\n\n\naxes = inspector.plot_partial_dependence(features=[\"bmi\"])\n\n\n\n\n\nax = inspector.plot_pred_vs_act()\n\n\n\n\n\nax = inspector.plot_residuals()\n\n\n\n\n\ninspector.show_correlation()\n\n\n\n\n\n\n \nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\nage\n1.00\n0.17\n0.19\n0.34\n0.26\n0.22\n-0.08\n0.20\n0.27\n0.30\n0.19\n\n\nsex\n0.17\n1.00\n0.09\n0.24\n0.04\n0.14\n-0.38\n0.33\n0.15\n0.21\n0.04\n\n\nbmi\n0.19\n0.09\n1.00\n0.40\n0.25\n0.26\n-0.37\n0.41\n0.45\n0.39\n0.59\n\n\nbp\n0.34\n0.24\n0.40\n1.00\n0.24\n0.19\n-0.18\n0.26\n0.39\n0.39\n0.44\n\n\ns1\n0.26\n0.04\n0.25\n0.24\n1.00\n0.90\n0.05\n0.54\n0.52\n0.33\n0.21\n\n\ns2\n0.22\n0.14\n0.26\n0.19\n0.90\n1.00\n-0.20\n0.66\n0.32\n0.29\n0.17\n\n\ns3\n-0.08\n-0.38\n-0.37\n-0.18\n0.05\n-0.20\n1.00\n-0.74\n-0.40\n-0.27\n-0.39\n\n\ns4\n0.20\n0.33\n0.41\n0.26\n0.54\n0.66\n-0.74\n1.00\n0.62\n0.42\n0.43\n\n\ns5\n0.27\n0.15\n0.45\n0.39\n0.52\n0.32\n-0.40\n0.62\n1.00\n0.46\n0.57\n\n\ns6\n0.30\n0.21\n0.39\n0.39\n0.33\n0.29\n-0.27\n0.42\n0.46\n1.00\n0.38\n\n\ntarget\n0.19\n0.04\n0.59\n0.44\n0.21\n0.17\n-0.39\n0.43\n0.57\n0.38\n1.00\n\n\n\n\n\n\n\nLGBMClassifier\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n\n\ninspector = get_inspector(LGBMClassifier(n_estimators=5).fit(X, y), X, y)\n\nLearning rate set to 0.5\n0:  learn: 0.2068913    total: 2.79ms   remaining: 11.2ms\n1:  learn: 0.1073211    total: 4.88ms   remaining: 7.32ms\n2:  learn: 0.0762370    total: 6.91ms   remaining: 4.61ms\n3:  learn: 0.0572005    total: 8.96ms   remaining: 2.24ms\n4:  learn: 0.0464524    total: 15.5ms   remaining: 0us\n\n\n\ninspector.methods\n\n['calculate_metrics_by_thresh',\n 'confusion_matrix',\n 'permutation_importance',\n 'plot_feature_clusters',\n 'plot_partial_dependence',\n 'plot_permutation_importance',\n 'show_correlation']\n\n\n\nax = inspector.calculate_metrics_by_thresh(\n    sklearn.metrics.accuracy_score, np.linspace(0, 1, 11)\n).plot(x=\"thresh\")\n\n100%|██████████| 11/11 [00:00&lt;00:00, 2964.93it/s]\n\n\n\n\n\n\ninspector.confusion_matrix()\n\n\n\n\n\n\n \nPredicted 0\nPredicted 1\nTotals\n\n\n\n\nActual 0\n208\n4\n212\n\n\nActual 1\n0\n357\n357\n\n\nTotals\n208\n361\n569\n\n\n\n\n\n\ninspector.permutation_importance()\n\nworst area                 0.069947\nmean texture               0.024605\nworst smoothness           0.016872\nworst concavity            0.014411\nworst concave points       0.009490\nmean radius                0.008436\nmean smoothness            0.004921\nmean area                  0.003866\nmean concavity             0.003515\nperimeter error            0.003163\nmean concave points        0.003163\narea error                 0.002812\ncompactness error          0.002812\nworst fractal dimension    0.002109\nworst perimeter            0.002109\nworst texture              0.001757\nmean perimeter             0.001054\nworst compactness          0.000351\nconcavity error            0.000000\nconcave points error       0.000000\nsymmetry error             0.000000\nfractal dimension error    0.000000\nworst radius               0.000000\nsmoothness error           0.000000\nradius error               0.000000\nmean fractal dimension     0.000000\nmean symmetry              0.000000\nmean compactness           0.000000\nworst symmetry             0.000000\ntexture error              0.000000\ndtype: float64\n\n\n\nax = inspector.plot_feature_clusters()\n\n\n\n\n\naxes = inspector.plot_partial_dependence(features=[\"worst texture\"])\n\n\n\n\n\nax = inspector.plot_permutation_importance()\n\n\n\n\n\ninspector.show_correlation()\n\n\n\n\n\n\n \nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\nradius error\ntexture error\nperimeter error\narea error\nsmoothness error\ncompactness error\nconcavity error\nconcave points error\nsymmetry error\nfractal dimension error\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\nmean radius\n1.00\n0.32\n1.00\n0.99\n0.17\n0.51\n0.68\n0.82\n0.15\n-0.31\n0.68\n-0.10\n0.67\n0.74\n-0.22\n0.21\n0.19\n0.38\n-0.10\n-0.04\n0.97\n0.30\n0.97\n0.94\n0.12\n0.41\n0.53\n0.74\n0.16\n0.01\n-0.73\n\n\nmean texture\n0.32\n1.00\n0.33\n0.32\n-0.02\n0.24\n0.30\n0.29\n0.07\n-0.08\n0.28\n0.39\n0.28\n0.26\n0.01\n0.19\n0.14\n0.16\n0.01\n0.05\n0.35\n0.91\n0.36\n0.34\n0.08\n0.28\n0.30\n0.30\n0.11\n0.12\n-0.42\n\n\nmean perimeter\n1.00\n0.33\n1.00\n0.99\n0.21\n0.56\n0.72\n0.85\n0.18\n-0.26\n0.69\n-0.09\n0.69\n0.74\n-0.20\n0.25\n0.23\n0.41\n-0.08\n-0.01\n0.97\n0.30\n0.97\n0.94\n0.15\n0.46\n0.56\n0.77\n0.19\n0.05\n-0.74\n\n\nmean area\n0.99\n0.32\n0.99\n1.00\n0.18\n0.50\n0.69\n0.82\n0.15\n-0.28\n0.73\n-0.07\n0.73\n0.80\n-0.17\n0.21\n0.21\n0.37\n-0.07\n-0.02\n0.96\n0.29\n0.96\n0.96\n0.12\n0.39\n0.51\n0.72\n0.14\n0.00\n-0.71\n\n\nmean smoothness\n0.17\n-0.02\n0.21\n0.18\n1.00\n0.66\n0.52\n0.55\n0.56\n0.58\n0.30\n0.07\n0.30\n0.25\n0.33\n0.32\n0.25\n0.38\n0.20\n0.28\n0.21\n0.04\n0.24\n0.21\n0.81\n0.47\n0.43\n0.50\n0.39\n0.50\n-0.36\n\n\nmean compactness\n0.51\n0.24\n0.56\n0.50\n0.66\n1.00\n0.88\n0.83\n0.60\n0.57\n0.50\n0.05\n0.55\n0.46\n0.14\n0.74\n0.57\n0.64\n0.23\n0.51\n0.54\n0.25\n0.59\n0.51\n0.57\n0.87\n0.82\n0.82\n0.51\n0.69\n-0.60\n\n\nmean concavity\n0.68\n0.30\n0.72\n0.69\n0.52\n0.88\n1.00\n0.92\n0.50\n0.34\n0.63\n0.08\n0.66\n0.62\n0.10\n0.67\n0.69\n0.68\n0.18\n0.45\n0.69\n0.30\n0.73\n0.68\n0.45\n0.75\n0.88\n0.86\n0.41\n0.51\n-0.70\n\n\nmean concave points\n0.82\n0.29\n0.85\n0.82\n0.55\n0.83\n0.92\n1.00\n0.46\n0.17\n0.70\n0.02\n0.71\n0.69\n0.03\n0.49\n0.44\n0.62\n0.10\n0.26\n0.83\n0.29\n0.86\n0.81\n0.45\n0.67\n0.75\n0.91\n0.38\n0.37\n-0.78\n\n\nmean symmetry\n0.15\n0.07\n0.18\n0.15\n0.56\n0.60\n0.50\n0.46\n1.00\n0.48\n0.30\n0.13\n0.31\n0.22\n0.19\n0.42\n0.34\n0.39\n0.45\n0.33\n0.19\n0.09\n0.22\n0.18\n0.43\n0.47\n0.43\n0.43\n0.70\n0.44\n-0.33\n\n\nmean fractal dimension\n-0.31\n-0.08\n-0.26\n-0.28\n0.58\n0.57\n0.34\n0.17\n0.48\n1.00\n0.00\n0.16\n0.04\n-0.09\n0.40\n0.56\n0.45\n0.34\n0.35\n0.69\n-0.25\n-0.05\n-0.21\n-0.23\n0.50\n0.46\n0.35\n0.18\n0.33\n0.77\n0.01\n\n\nradius error\n0.68\n0.28\n0.69\n0.73\n0.30\n0.50\n0.63\n0.70\n0.30\n0.00\n1.00\n0.21\n0.97\n0.95\n0.16\n0.36\n0.33\n0.51\n0.24\n0.23\n0.72\n0.19\n0.72\n0.75\n0.14\n0.29\n0.38\n0.53\n0.09\n0.05\n-0.57\n\n\ntexture error\n-0.10\n0.39\n-0.09\n-0.07\n0.07\n0.05\n0.08\n0.02\n0.13\n0.16\n0.21\n1.00\n0.22\n0.11\n0.40\n0.23\n0.19\n0.23\n0.41\n0.28\n-0.11\n0.41\n-0.10\n-0.08\n-0.07\n-0.09\n-0.07\n-0.12\n-0.13\n-0.05\n0.01\n\n\nperimeter error\n0.67\n0.28\n0.69\n0.73\n0.30\n0.55\n0.66\n0.71\n0.31\n0.04\n0.97\n0.22\n1.00\n0.94\n0.15\n0.42\n0.36\n0.56\n0.27\n0.24\n0.70\n0.20\n0.72\n0.73\n0.13\n0.34\n0.42\n0.55\n0.11\n0.09\n-0.56\n\n\narea error\n0.74\n0.26\n0.74\n0.80\n0.25\n0.46\n0.62\n0.69\n0.22\n-0.09\n0.95\n0.11\n0.94\n1.00\n0.08\n0.28\n0.27\n0.42\n0.13\n0.13\n0.76\n0.20\n0.76\n0.81\n0.13\n0.28\n0.39\n0.54\n0.07\n0.02\n-0.55\n\n\nsmoothness error\n-0.22\n0.01\n-0.20\n-0.17\n0.33\n0.14\n0.10\n0.03\n0.19\n0.40\n0.16\n0.40\n0.15\n0.08\n1.00\n0.34\n0.27\n0.33\n0.41\n0.43\n-0.23\n-0.07\n-0.22\n-0.18\n0.31\n-0.06\n-0.06\n-0.10\n-0.11\n0.10\n0.07\n\n\ncompactness error\n0.21\n0.19\n0.25\n0.21\n0.32\n0.74\n0.67\n0.49\n0.42\n0.56\n0.36\n0.23\n0.42\n0.28\n0.34\n1.00\n0.80\n0.74\n0.39\n0.80\n0.20\n0.14\n0.26\n0.20\n0.23\n0.68\n0.64\n0.48\n0.28\n0.59\n-0.29\n\n\nconcavity error\n0.19\n0.14\n0.23\n0.21\n0.25\n0.57\n0.69\n0.44\n0.34\n0.45\n0.33\n0.19\n0.36\n0.27\n0.27\n0.80\n1.00\n0.77\n0.31\n0.73\n0.19\n0.10\n0.23\n0.19\n0.17\n0.48\n0.66\n0.44\n0.20\n0.44\n-0.25\n\n\nconcave points error\n0.38\n0.16\n0.41\n0.37\n0.38\n0.64\n0.68\n0.62\n0.39\n0.34\n0.51\n0.23\n0.56\n0.42\n0.33\n0.74\n0.77\n1.00\n0.31\n0.61\n0.36\n0.09\n0.39\n0.34\n0.22\n0.45\n0.55\n0.60\n0.14\n0.31\n-0.41\n\n\nsymmetry error\n-0.10\n0.01\n-0.08\n-0.07\n0.20\n0.23\n0.18\n0.10\n0.45\n0.35\n0.24\n0.41\n0.27\n0.13\n0.41\n0.39\n0.31\n0.31\n1.00\n0.37\n-0.13\n-0.08\n-0.10\n-0.11\n-0.01\n0.06\n0.04\n-0.03\n0.39\n0.08\n0.01\n\n\nfractal dimension error\n-0.04\n0.05\n-0.01\n-0.02\n0.28\n0.51\n0.45\n0.26\n0.33\n0.69\n0.23\n0.28\n0.24\n0.13\n0.43\n0.80\n0.73\n0.61\n0.37\n1.00\n-0.04\n-0.00\n-0.00\n-0.02\n0.17\n0.39\n0.38\n0.22\n0.11\n0.59\n-0.08\n\n\nworst radius\n0.97\n0.35\n0.97\n0.96\n0.21\n0.54\n0.69\n0.83\n0.19\n-0.25\n0.72\n-0.11\n0.70\n0.76\n-0.23\n0.20\n0.19\n0.36\n-0.13\n-0.04\n1.00\n0.36\n0.99\n0.98\n0.22\n0.48\n0.57\n0.79\n0.24\n0.09\n-0.78\n\n\nworst texture\n0.30\n0.91\n0.30\n0.29\n0.04\n0.25\n0.30\n0.29\n0.09\n-0.05\n0.19\n0.41\n0.20\n0.20\n-0.07\n0.14\n0.10\n0.09\n-0.08\n-0.00\n0.36\n1.00\n0.37\n0.35\n0.23\n0.36\n0.37\n0.36\n0.23\n0.22\n-0.46\n\n\nworst perimeter\n0.97\n0.36\n0.97\n0.96\n0.24\n0.59\n0.73\n0.86\n0.22\n-0.21\n0.72\n-0.10\n0.72\n0.76\n-0.22\n0.26\n0.23\n0.39\n-0.10\n-0.00\n0.99\n0.37\n1.00\n0.98\n0.24\n0.53\n0.62\n0.82\n0.27\n0.14\n-0.78\n\n\nworst area\n0.94\n0.34\n0.94\n0.96\n0.21\n0.51\n0.68\n0.81\n0.18\n-0.23\n0.75\n-0.08\n0.73\n0.81\n-0.18\n0.20\n0.19\n0.34\n-0.11\n-0.02\n0.98\n0.35\n0.98\n1.00\n0.21\n0.44\n0.54\n0.75\n0.21\n0.08\n-0.73\n\n\nworst smoothness\n0.12\n0.08\n0.15\n0.12\n0.81\n0.57\n0.45\n0.45\n0.43\n0.50\n0.14\n-0.07\n0.13\n0.13\n0.31\n0.23\n0.17\n0.22\n-0.01\n0.17\n0.22\n0.23\n0.24\n0.21\n1.00\n0.57\n0.52\n0.55\n0.49\n0.62\n-0.42\n\n\nworst compactness\n0.41\n0.28\n0.46\n0.39\n0.47\n0.87\n0.75\n0.67\n0.47\n0.46\n0.29\n-0.09\n0.34\n0.28\n-0.06\n0.68\n0.48\n0.45\n0.06\n0.39\n0.48\n0.36\n0.53\n0.44\n0.57\n1.00\n0.89\n0.80\n0.61\n0.81\n-0.59\n\n\nworst concavity\n0.53\n0.30\n0.56\n0.51\n0.43\n0.82\n0.88\n0.75\n0.43\n0.35\n0.38\n-0.07\n0.42\n0.39\n-0.06\n0.64\n0.66\n0.55\n0.04\n0.38\n0.57\n0.37\n0.62\n0.54\n0.52\n0.89\n1.00\n0.86\n0.53\n0.69\n-0.66\n\n\nworst concave points\n0.74\n0.30\n0.77\n0.72\n0.50\n0.82\n0.86\n0.91\n0.43\n0.18\n0.53\n-0.12\n0.55\n0.54\n-0.10\n0.48\n0.44\n0.60\n-0.03\n0.22\n0.79\n0.36\n0.82\n0.75\n0.55\n0.80\n0.86\n1.00\n0.50\n0.51\n-0.79\n\n\nworst symmetry\n0.16\n0.11\n0.19\n0.14\n0.39\n0.51\n0.41\n0.38\n0.70\n0.33\n0.09\n-0.13\n0.11\n0.07\n-0.11\n0.28\n0.20\n0.14\n0.39\n0.11\n0.24\n0.23\n0.27\n0.21\n0.49\n0.61\n0.53\n0.50\n1.00\n0.54\n-0.42\n\n\nworst fractal dimension\n0.01\n0.12\n0.05\n0.00\n0.50\n0.69\n0.51\n0.37\n0.44\n0.77\n0.05\n-0.05\n0.09\n0.02\n0.10\n0.59\n0.44\n0.31\n0.08\n0.59\n0.09\n0.22\n0.14\n0.08\n0.62\n0.81\n0.69\n0.51\n0.54\n1.00\n-0.32\n\n\ntarget\n-0.73\n-0.42\n-0.74\n-0.71\n-0.36\n-0.60\n-0.70\n-0.78\n-0.33\n0.01\n-0.57\n0.01\n-0.56\n-0.55\n0.07\n-0.29\n-0.25\n-0.41\n0.01\n-0.08\n-0.78\n-0.46\n-0.78\n-0.73\n-0.42\n-0.59\n-0.66\n-0.79\n-0.42\n-0.32\n1.00"
  },
  {
    "objectID": "gbm.html#xgboost",
    "href": "gbm.html#xgboost",
    "title": "GBMs",
    "section": "XGBoost",
    "text": "XGBoost\n\nXGBRegressor\n\nX, y = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n\n\ninspector = get_inspector(XGBRegressor(n_estimators=5).fit(X, y), X, y)\n\n\ninspector.methods\n\n['permutation_importance',\n 'plot_feature_clusters',\n 'plot_partial_dependence',\n 'plot_permutation_importance',\n 'plot_pred_vs_act',\n 'plot_residuals',\n 'show_correlation']\n\n\n\ninspector.permutation_importance()\n\ns5     0.649371\nbmi    0.445589\nbp     0.137778\ns6     0.103405\nage    0.079496\ns2     0.074219\ns3     0.070875\ns1     0.049026\nsex    0.022239\ns4     0.008080\ndtype: float64\n\n\n\nax = inspector.plot_feature_clusters()\n\n\n\n\n\naxes = inspector.plot_partial_dependence(features=[\"bmi\"])\n\n\n\n\n\nax = inspector.plot_pred_vs_act()\n\n\n\n\n\naxes = inspector.plot_residuals()\n\n\n\n\n\ninspector.show_correlation()\n\n\n\n\n\n\n \nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\nage\n1.00\n0.17\n0.19\n0.34\n0.26\n0.22\n-0.08\n0.20\n0.27\n0.30\n0.19\n\n\nsex\n0.17\n1.00\n0.09\n0.24\n0.04\n0.14\n-0.38\n0.33\n0.15\n0.21\n0.04\n\n\nbmi\n0.19\n0.09\n1.00\n0.40\n0.25\n0.26\n-0.37\n0.41\n0.45\n0.39\n0.59\n\n\nbp\n0.34\n0.24\n0.40\n1.00\n0.24\n0.19\n-0.18\n0.26\n0.39\n0.39\n0.44\n\n\ns1\n0.26\n0.04\n0.25\n0.24\n1.00\n0.90\n0.05\n0.54\n0.52\n0.33\n0.21\n\n\ns2\n0.22\n0.14\n0.26\n0.19\n0.90\n1.00\n-0.20\n0.66\n0.32\n0.29\n0.17\n\n\ns3\n-0.08\n-0.38\n-0.37\n-0.18\n0.05\n-0.20\n1.00\n-0.74\n-0.40\n-0.27\n-0.39\n\n\ns4\n0.20\n0.33\n0.41\n0.26\n0.54\n0.66\n-0.74\n1.00\n0.62\n0.42\n0.43\n\n\ns5\n0.27\n0.15\n0.45\n0.39\n0.52\n0.32\n-0.40\n0.62\n1.00\n0.46\n0.57\n\n\ns6\n0.30\n0.21\n0.39\n0.39\n0.33\n0.29\n-0.27\n0.42\n0.46\n1.00\n0.38\n\n\ntarget\n0.19\n0.04\n0.59\n0.44\n0.21\n0.17\n-0.39\n0.43\n0.57\n0.38\n1.00\n\n\n\n\n\n\n\nXGBClassifier\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\n\n\ninspector = get_inspector(XGBClassifier(n_estimators=5).fit(X, y), X, y)\n\n\ninspector.methods\n\n['calculate_metrics_by_thresh',\n 'confusion_matrix',\n 'permutation_importance',\n 'plot_feature_clusters',\n 'plot_partial_dependence',\n 'plot_permutation_importance',\n 'show_correlation']\n\n\n\nax = inspector.calculate_metrics_by_thresh(\n    sklearn.metrics.accuracy_score, np.linspace(0, 1, 11)\n).plot(x=\"thresh\")\n\n100%|██████████| 11/11 [00:00&lt;00:00, 3486.54it/s]\n\n\n\n\n\n\ninspector.confusion_matrix()\n\n\n\n\n\n\n \nPredicted 0\nPredicted 1\nTotals\n\n\n\n\nActual 0\n209\n3\n212\n\n\nActual 1\n0\n357\n357\n\n\nTotals\n209\n360\n569\n\n\n\n\n\n\ninspector.permutation_importance()\n\nworst concave points       0.071353\nworst radius               0.031283\nmean concave points        0.017926\nmean texture               0.017926\nworst texture              0.013708\nworst area                 0.007381\nworst perimeter            0.004921\nworst concavity            0.003515\nmean radius                0.001757\narea error                 0.001757\nmean area                  0.001757\nradius error               0.001757\nmean concavity             0.001054\nconcave points error       0.000703\nfractal dimension error    0.000000\nworst compactness          0.000000\nworst symmetry             0.000000\nworst smoothness           0.000000\ncompactness error          0.000000\nsymmetry error             0.000000\nconcavity error            0.000000\nsmoothness error           0.000000\nperimeter error            0.000000\ntexture error              0.000000\nmean fractal dimension     0.000000\nmean symmetry              0.000000\nmean compactness           0.000000\nmean smoothness            0.000000\nmean perimeter             0.000000\nworst fractal dimension    0.000000\ndtype: float64\n\n\n\nax = inspector.plot_feature_clusters()\n\n\n\n\n\naxes = inspector.plot_partial_dependence(features=[\"worst texture\"])\n\n\n\n\n\nax = inspector.plot_permutation_importance()\n\n\n\n\n\ninspector.show_correlation()\n\n\n\n\n\n\n \nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\nradius error\ntexture error\nperimeter error\narea error\nsmoothness error\ncompactness error\nconcavity error\nconcave points error\nsymmetry error\nfractal dimension error\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\nmean radius\n1.00\n0.32\n1.00\n0.99\n0.17\n0.51\n0.68\n0.82\n0.15\n-0.31\n0.68\n-0.10\n0.67\n0.74\n-0.22\n0.21\n0.19\n0.38\n-0.10\n-0.04\n0.97\n0.30\n0.97\n0.94\n0.12\n0.41\n0.53\n0.74\n0.16\n0.01\n-0.73\n\n\nmean texture\n0.32\n1.00\n0.33\n0.32\n-0.02\n0.24\n0.30\n0.29\n0.07\n-0.08\n0.28\n0.39\n0.28\n0.26\n0.01\n0.19\n0.14\n0.16\n0.01\n0.05\n0.35\n0.91\n0.36\n0.34\n0.08\n0.28\n0.30\n0.30\n0.11\n0.12\n-0.42\n\n\nmean perimeter\n1.00\n0.33\n1.00\n0.99\n0.21\n0.56\n0.72\n0.85\n0.18\n-0.26\n0.69\n-0.09\n0.69\n0.74\n-0.20\n0.25\n0.23\n0.41\n-0.08\n-0.01\n0.97\n0.30\n0.97\n0.94\n0.15\n0.46\n0.56\n0.77\n0.19\n0.05\n-0.74\n\n\nmean area\n0.99\n0.32\n0.99\n1.00\n0.18\n0.50\n0.69\n0.82\n0.15\n-0.28\n0.73\n-0.07\n0.73\n0.80\n-0.17\n0.21\n0.21\n0.37\n-0.07\n-0.02\n0.96\n0.29\n0.96\n0.96\n0.12\n0.39\n0.51\n0.72\n0.14\n0.00\n-0.71\n\n\nmean smoothness\n0.17\n-0.02\n0.21\n0.18\n1.00\n0.66\n0.52\n0.55\n0.56\n0.58\n0.30\n0.07\n0.30\n0.25\n0.33\n0.32\n0.25\n0.38\n0.20\n0.28\n0.21\n0.04\n0.24\n0.21\n0.81\n0.47\n0.43\n0.50\n0.39\n0.50\n-0.36\n\n\nmean compactness\n0.51\n0.24\n0.56\n0.50\n0.66\n1.00\n0.88\n0.83\n0.60\n0.57\n0.50\n0.05\n0.55\n0.46\n0.14\n0.74\n0.57\n0.64\n0.23\n0.51\n0.54\n0.25\n0.59\n0.51\n0.57\n0.87\n0.82\n0.82\n0.51\n0.69\n-0.60\n\n\nmean concavity\n0.68\n0.30\n0.72\n0.69\n0.52\n0.88\n1.00\n0.92\n0.50\n0.34\n0.63\n0.08\n0.66\n0.62\n0.10\n0.67\n0.69\n0.68\n0.18\n0.45\n0.69\n0.30\n0.73\n0.68\n0.45\n0.75\n0.88\n0.86\n0.41\n0.51\n-0.70\n\n\nmean concave points\n0.82\n0.29\n0.85\n0.82\n0.55\n0.83\n0.92\n1.00\n0.46\n0.17\n0.70\n0.02\n0.71\n0.69\n0.03\n0.49\n0.44\n0.62\n0.10\n0.26\n0.83\n0.29\n0.86\n0.81\n0.45\n0.67\n0.75\n0.91\n0.38\n0.37\n-0.78\n\n\nmean symmetry\n0.15\n0.07\n0.18\n0.15\n0.56\n0.60\n0.50\n0.46\n1.00\n0.48\n0.30\n0.13\n0.31\n0.22\n0.19\n0.42\n0.34\n0.39\n0.45\n0.33\n0.19\n0.09\n0.22\n0.18\n0.43\n0.47\n0.43\n0.43\n0.70\n0.44\n-0.33\n\n\nmean fractal dimension\n-0.31\n-0.08\n-0.26\n-0.28\n0.58\n0.57\n0.34\n0.17\n0.48\n1.00\n0.00\n0.16\n0.04\n-0.09\n0.40\n0.56\n0.45\n0.34\n0.35\n0.69\n-0.25\n-0.05\n-0.21\n-0.23\n0.50\n0.46\n0.35\n0.18\n0.33\n0.77\n0.01\n\n\nradius error\n0.68\n0.28\n0.69\n0.73\n0.30\n0.50\n0.63\n0.70\n0.30\n0.00\n1.00\n0.21\n0.97\n0.95\n0.16\n0.36\n0.33\n0.51\n0.24\n0.23\n0.72\n0.19\n0.72\n0.75\n0.14\n0.29\n0.38\n0.53\n0.09\n0.05\n-0.57\n\n\ntexture error\n-0.10\n0.39\n-0.09\n-0.07\n0.07\n0.05\n0.08\n0.02\n0.13\n0.16\n0.21\n1.00\n0.22\n0.11\n0.40\n0.23\n0.19\n0.23\n0.41\n0.28\n-0.11\n0.41\n-0.10\n-0.08\n-0.07\n-0.09\n-0.07\n-0.12\n-0.13\n-0.05\n0.01\n\n\nperimeter error\n0.67\n0.28\n0.69\n0.73\n0.30\n0.55\n0.66\n0.71\n0.31\n0.04\n0.97\n0.22\n1.00\n0.94\n0.15\n0.42\n0.36\n0.56\n0.27\n0.24\n0.70\n0.20\n0.72\n0.73\n0.13\n0.34\n0.42\n0.55\n0.11\n0.09\n-0.56\n\n\narea error\n0.74\n0.26\n0.74\n0.80\n0.25\n0.46\n0.62\n0.69\n0.22\n-0.09\n0.95\n0.11\n0.94\n1.00\n0.08\n0.28\n0.27\n0.42\n0.13\n0.13\n0.76\n0.20\n0.76\n0.81\n0.13\n0.28\n0.39\n0.54\n0.07\n0.02\n-0.55\n\n\nsmoothness error\n-0.22\n0.01\n-0.20\n-0.17\n0.33\n0.14\n0.10\n0.03\n0.19\n0.40\n0.16\n0.40\n0.15\n0.08\n1.00\n0.34\n0.27\n0.33\n0.41\n0.43\n-0.23\n-0.07\n-0.22\n-0.18\n0.31\n-0.06\n-0.06\n-0.10\n-0.11\n0.10\n0.07\n\n\ncompactness error\n0.21\n0.19\n0.25\n0.21\n0.32\n0.74\n0.67\n0.49\n0.42\n0.56\n0.36\n0.23\n0.42\n0.28\n0.34\n1.00\n0.80\n0.74\n0.39\n0.80\n0.20\n0.14\n0.26\n0.20\n0.23\n0.68\n0.64\n0.48\n0.28\n0.59\n-0.29\n\n\nconcavity error\n0.19\n0.14\n0.23\n0.21\n0.25\n0.57\n0.69\n0.44\n0.34\n0.45\n0.33\n0.19\n0.36\n0.27\n0.27\n0.80\n1.00\n0.77\n0.31\n0.73\n0.19\n0.10\n0.23\n0.19\n0.17\n0.48\n0.66\n0.44\n0.20\n0.44\n-0.25\n\n\nconcave points error\n0.38\n0.16\n0.41\n0.37\n0.38\n0.64\n0.68\n0.62\n0.39\n0.34\n0.51\n0.23\n0.56\n0.42\n0.33\n0.74\n0.77\n1.00\n0.31\n0.61\n0.36\n0.09\n0.39\n0.34\n0.22\n0.45\n0.55\n0.60\n0.14\n0.31\n-0.41\n\n\nsymmetry error\n-0.10\n0.01\n-0.08\n-0.07\n0.20\n0.23\n0.18\n0.10\n0.45\n0.35\n0.24\n0.41\n0.27\n0.13\n0.41\n0.39\n0.31\n0.31\n1.00\n0.37\n-0.13\n-0.08\n-0.10\n-0.11\n-0.01\n0.06\n0.04\n-0.03\n0.39\n0.08\n0.01\n\n\nfractal dimension error\n-0.04\n0.05\n-0.01\n-0.02\n0.28\n0.51\n0.45\n0.26\n0.33\n0.69\n0.23\n0.28\n0.24\n0.13\n0.43\n0.80\n0.73\n0.61\n0.37\n1.00\n-0.04\n-0.00\n-0.00\n-0.02\n0.17\n0.39\n0.38\n0.22\n0.11\n0.59\n-0.08\n\n\nworst radius\n0.97\n0.35\n0.97\n0.96\n0.21\n0.54\n0.69\n0.83\n0.19\n-0.25\n0.72\n-0.11\n0.70\n0.76\n-0.23\n0.20\n0.19\n0.36\n-0.13\n-0.04\n1.00\n0.36\n0.99\n0.98\n0.22\n0.48\n0.57\n0.79\n0.24\n0.09\n-0.78\n\n\nworst texture\n0.30\n0.91\n0.30\n0.29\n0.04\n0.25\n0.30\n0.29\n0.09\n-0.05\n0.19\n0.41\n0.20\n0.20\n-0.07\n0.14\n0.10\n0.09\n-0.08\n-0.00\n0.36\n1.00\n0.37\n0.35\n0.23\n0.36\n0.37\n0.36\n0.23\n0.22\n-0.46\n\n\nworst perimeter\n0.97\n0.36\n0.97\n0.96\n0.24\n0.59\n0.73\n0.86\n0.22\n-0.21\n0.72\n-0.10\n0.72\n0.76\n-0.22\n0.26\n0.23\n0.39\n-0.10\n-0.00\n0.99\n0.37\n1.00\n0.98\n0.24\n0.53\n0.62\n0.82\n0.27\n0.14\n-0.78\n\n\nworst area\n0.94\n0.34\n0.94\n0.96\n0.21\n0.51\n0.68\n0.81\n0.18\n-0.23\n0.75\n-0.08\n0.73\n0.81\n-0.18\n0.20\n0.19\n0.34\n-0.11\n-0.02\n0.98\n0.35\n0.98\n1.00\n0.21\n0.44\n0.54\n0.75\n0.21\n0.08\n-0.73\n\n\nworst smoothness\n0.12\n0.08\n0.15\n0.12\n0.81\n0.57\n0.45\n0.45\n0.43\n0.50\n0.14\n-0.07\n0.13\n0.13\n0.31\n0.23\n0.17\n0.22\n-0.01\n0.17\n0.22\n0.23\n0.24\n0.21\n1.00\n0.57\n0.52\n0.55\n0.49\n0.62\n-0.42\n\n\nworst compactness\n0.41\n0.28\n0.46\n0.39\n0.47\n0.87\n0.75\n0.67\n0.47\n0.46\n0.29\n-0.09\n0.34\n0.28\n-0.06\n0.68\n0.48\n0.45\n0.06\n0.39\n0.48\n0.36\n0.53\n0.44\n0.57\n1.00\n0.89\n0.80\n0.61\n0.81\n-0.59\n\n\nworst concavity\n0.53\n0.30\n0.56\n0.51\n0.43\n0.82\n0.88\n0.75\n0.43\n0.35\n0.38\n-0.07\n0.42\n0.39\n-0.06\n0.64\n0.66\n0.55\n0.04\n0.38\n0.57\n0.37\n0.62\n0.54\n0.52\n0.89\n1.00\n0.86\n0.53\n0.69\n-0.66\n\n\nworst concave points\n0.74\n0.30\n0.77\n0.72\n0.50\n0.82\n0.86\n0.91\n0.43\n0.18\n0.53\n-0.12\n0.55\n0.54\n-0.10\n0.48\n0.44\n0.60\n-0.03\n0.22\n0.79\n0.36\n0.82\n0.75\n0.55\n0.80\n0.86\n1.00\n0.50\n0.51\n-0.79\n\n\nworst symmetry\n0.16\n0.11\n0.19\n0.14\n0.39\n0.51\n0.41\n0.38\n0.70\n0.33\n0.09\n-0.13\n0.11\n0.07\n-0.11\n0.28\n0.20\n0.14\n0.39\n0.11\n0.24\n0.23\n0.27\n0.21\n0.49\n0.61\n0.53\n0.50\n1.00\n0.54\n-0.42\n\n\nworst fractal dimension\n0.01\n0.12\n0.05\n0.00\n0.50\n0.69\n0.51\n0.37\n0.44\n0.77\n0.05\n-0.05\n0.09\n0.02\n0.10\n0.59\n0.44\n0.31\n0.08\n0.59\n0.09\n0.22\n0.14\n0.08\n0.62\n0.81\n0.69\n0.51\n0.54\n1.00\n-0.32\n\n\ntarget\n-0.73\n-0.42\n-0.74\n-0.71\n-0.36\n-0.60\n-0.70\n-0.78\n-0.33\n0.01\n-0.57\n0.01\n-0.56\n-0.55\n0.07\n-0.29\n-0.25\n-0.41\n0.01\n-0.08\n-0.78\n-0.46\n-0.78\n-0.73\n-0.42\n-0.59\n-0.66\n-0.79\n-0.42\n-0.32\n1.00"
  },
  {
    "objectID": "linear_model.html",
    "href": "linear_model.html",
    "title": "Linear Model",
    "section": "",
    "text": "import sklearn.datasets\nfrom model_inspector import get_inspector\nfrom sklearn.linear_model import Ridge\n\n\nX, y = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n\ninspector = get_inspector(Ridge().fit(X, y), X, y)\n\n\nsource\n\n\n\n plot_coefs_vs_hparam (regression model) (hparam:str,\n                       vals:Sequence[float])\n\nPlot coefficient values against a hyperparameter\nParameters:\n\nhparam: Name of hyperparameter; must be an attribute of self.model\nvals: Values of that hyperparameter to use\n\n\nax = inspector.plot_coefs_vs_hparam(\"alpha\", np.logspace(-2, 0.5, 10))\n\n\n\n\n\nsource\n\n\n\n\n plot_waterfall (regression model)\n                 (item:Union[pandas.core.series.Series,&lt;built-\n                 infunctionarray&gt;], bar_num_formatter:str='.1f',\n                 tick_num_formatter:str='.2f', sorted_value=True,\n                 threshold=0.01, blue_color='#377eb8',\n                 green_color='#4daf4a', red_color='#ff7f00', Title='',\n                 x_lab='', y_lab='', formatting='{:,.1f}',\n                 other_label='other', net_label='net', rotation_value=30)\n\nMake a waterfall chart showing how each feature contributes to the prediction for the input item.\nParameters:\n\nitem: Input item, with the same shape and value meanings as a single row from self.X\nbar_num_formatter: Bar label format specifier\ntick_num_formatter: Tick label format specifier\n\nAdditional keyword arguments will be passed to waterfall_chart.plot\nRemaining parameters are passed to waterfall_chart.plot.\n\nax = inspector.plot_waterfall(X.iloc[0])\n\n\n\n\n\nsource\n\n\n\n\n show_model (regression model) (intercept_formatter:str='.2f',\n             coef_formatter:str='.2f')\n\nShow model equation\nParameters:\n\nintercept_formatter: Intercept format specifier\ncoef_formatter: Intercept format specifier\n\n\ninspector.show_model()\n\n\n            target\n            = 152.13\n        \n                + 29.47\n                * age\n            \n                - 83.15\n                * sex\n            \n                + 306.35\n                * bmi\n            \n                + 201.63\n                * bp\n            \n                + 5.91\n                * s1\n            \n                - 29.52\n                * s2\n            \n                - 152.04\n                * s3\n            \n                + 117.31\n                * s4\n            \n                + 262.94\n                * s5\n            \n                + 111.88\n                * s6"
  },
  {
    "objectID": "linear_model.html#linear-regression",
    "href": "linear_model.html#linear-regression",
    "title": "Linear Model",
    "section": "",
    "text": "import sklearn.datasets\nfrom model_inspector import get_inspector\nfrom sklearn.linear_model import Ridge\n\n\nX, y = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n\ninspector = get_inspector(Ridge().fit(X, y), X, y)\n\n\nsource\n\n\n\n plot_coefs_vs_hparam (regression model) (hparam:str,\n                       vals:Sequence[float])\n\nPlot coefficient values against a hyperparameter\nParameters:\n\nhparam: Name of hyperparameter; must be an attribute of self.model\nvals: Values of that hyperparameter to use\n\n\nax = inspector.plot_coefs_vs_hparam(\"alpha\", np.logspace(-2, 0.5, 10))\n\n\n\n\n\nsource\n\n\n\n\n plot_waterfall (regression model)\n                 (item:Union[pandas.core.series.Series,&lt;built-\n                 infunctionarray&gt;], bar_num_formatter:str='.1f',\n                 tick_num_formatter:str='.2f', sorted_value=True,\n                 threshold=0.01, blue_color='#377eb8',\n                 green_color='#4daf4a', red_color='#ff7f00', Title='',\n                 x_lab='', y_lab='', formatting='{:,.1f}',\n                 other_label='other', net_label='net', rotation_value=30)\n\nMake a waterfall chart showing how each feature contributes to the prediction for the input item.\nParameters:\n\nitem: Input item, with the same shape and value meanings as a single row from self.X\nbar_num_formatter: Bar label format specifier\ntick_num_formatter: Tick label format specifier\n\nAdditional keyword arguments will be passed to waterfall_chart.plot\nRemaining parameters are passed to waterfall_chart.plot.\n\nax = inspector.plot_waterfall(X.iloc[0])\n\n\n\n\n\nsource\n\n\n\n\n show_model (regression model) (intercept_formatter:str='.2f',\n             coef_formatter:str='.2f')\n\nShow model equation\nParameters:\n\nintercept_formatter: Intercept format specifier\ncoef_formatter: Intercept format specifier\n\n\ninspector.show_model()\n\n\n            target\n            = 152.13\n        \n                + 29.47\n                * age\n            \n                - 83.15\n                * sex\n            \n                + 306.35\n                * bmi\n            \n                + 201.63\n                * bp\n            \n                + 5.91\n                * s1\n            \n                - 29.52\n                * s2\n            \n                - 152.04\n                * s3\n            \n                + 117.31\n                * s4\n            \n                + 262.94\n                * s5\n            \n                + 111.88\n                * s6"
  },
  {
    "objectID": "linear_model.html#binary-logistic-regression",
    "href": "linear_model.html#binary-logistic-regression",
    "title": "Linear Model",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)\nX = X.iloc[:, :10]\n\n\ninspector = get_inspector(LogisticRegression(max_iter=1_000).fit(X, y), X, y)\n\n\nsource\n\nplot_coefs_vs_hparam (binary model)\n\n plot_coefs_vs_hparam (binary model) (hparam:str, vals:Sequence[float])\n\nPlot coefficient values against a hyperparameter\nParameters:\n\nhparam: Name of hyperparameter; must be an attribute of self.model\nvals: Values of that hyperparameter to use\n\n\nax = inspector.plot_coefs_vs_hparam(\"C\", np.logspace(-2, 0.5, 10))\n\n\n\n\n\nsource\n\n\nshow_model (binary model)\n\n show_model (binary model) (intercept_formatter:str='.2f',\n             coef_formatter:str='.2f')\n\nShow model equation\nParameters:\n\nintercept_formatter: Intercept format specifier\ncoef_formatter: Intercept format specifier\n\n\ninspector.show_model()\n\n\n            log-odds(target)\n            = 21.27\n        \n                + 2.69\n                * mean radius\n            \n                - 0.23\n                * mean texture\n            \n                - 0.61\n                * mean perimeter\n            \n                + 0.00\n                * mean area\n            \n                - 0.48\n                * mean smoothness\n            \n                - 0.79\n                * mean compactness\n            \n                - 1.42\n                * mean concavity\n            \n                - 0.76\n                * mean concave points\n            \n                - 0.69\n                * mean symmetry\n            \n                - 0.12\n                * mean fractal dimension\n            \n\n\n\nsource\n\n\nplot_waterfall (binary model)\n\n plot_waterfall (binary model)\n                 (item:Union[pandas.core.series.Series,&lt;built-\n                 infunctionarray&gt;], bar_num_formatter:str='.1f',\n                 tick_num_formatter:str='.2f', sorted_value=True,\n                 threshold=0.01, blue_color='#377eb8',\n                 green_color='#4daf4a', red_color='#ff7f00', Title='',\n                 x_lab='', y_lab='', formatting='{:,.1f}',\n                 other_label='other', net_label='net', rotation_value=30)\n\nMake a waterfall chart showing how each feature contributes to the prediction for the input item for a binary classification model.\nParameters:\n\nitem: Input item, with the same shape and value meanings as a single row from self.X\nbar_num_formatter: Bar label format specifier\ntick_num_formatter: Tick label format specifier\nwaterfall_kwargs: kwargs to pass to waterfall_chart.plot\n\nRemaining parameters are passed to waterfall_chart.plot.\n\nax = inspector.plot_waterfall(X.iloc[0])"
  },
  {
    "objectID": "linear_model.html#multiclass-logistic-regression",
    "href": "linear_model.html#multiclass-logistic-regression",
    "title": "Linear Model",
    "section": "Multiclass Logistic Regression",
    "text": "Multiclass Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nX, y = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\n\n\ninspector = get_inspector(LogisticRegression(max_iter=10_000).fit(X, y), X, y)\n\n\nsource\n\nplot_coefs_vs_hparam (multiclass model)\n\n plot_coefs_vs_hparam (multiclass model) (hparam:str,\n                       vals:Sequence[float])\n\nPlot coefficient values against a hyperparameter\nParameters:\n\nhparam: Name of hyperparameter; must be an attribute of self.model\nvals: Values of that hyperparameter to use\n\nReturns NumPy array of Axes objects.\n\naxes = inspector.plot_coefs_vs_hparam(\"C\", np.logspace(-2, 0.5, 10))\n\n\n\n\n\nsource\n\n\nshow_model (multiclass model)\n\n show_model (multiclass model) (intercept_formatter:str='.2f',\n             coef_formatter:str='.2f')\n\nShow model equation\nParameters:\n\nintercept_formatter: Intercept format specifier\ncoef_formatter: Intercept format specifier\n\n\ninspector.show_model()\n\n\n                    \n                        \n            log-odds(target = 0)\n            = 9.85\n        \n                - 0.42\n                * sepal length (cm)\n            \n                + 0.97\n                * sepal width (cm)\n            \n                - 2.52\n                * petal length (cm)\n            \n                - 1.08\n                * petal width (cm)\n            \n                    \n                \n                    \n                        \n            log-odds(target = 1)\n            = 2.24\n        \n                + 0.53\n                * sepal length (cm)\n            \n                - 0.32\n                * sepal width (cm)\n            \n                - 0.21\n                * petal length (cm)\n            \n                - 0.94\n                * petal width (cm)\n            \n                    \n                \n                    \n                        \n            log-odds(target = 2)\n            = -12.09\n        \n                - 0.11\n                * sepal length (cm)\n            \n                - 0.65\n                * sepal width (cm)\n            \n                + 2.72\n                * petal length (cm)\n            \n                + 2.02\n                * petal width (cm)"
  },
  {
    "objectID": "any_model.html",
    "href": "any_model.html",
    "title": "Any Model",
    "section": "",
    "text": "import sklearn.datasets\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom model_inspector import get_inspector\n\n\nX_diabetes, y_diabetes = sklearn.datasets.load_diabetes(return_X_y=True, as_frame=True)\n\ninspector = get_inspector(\n    RandomForestRegressor().fit(X_diabetes, y_diabetes), X_diabetes, y_diabetes\n)\n\n/Users/greg.gandenberger/repos/model_inspector/model_inspector/inspect/any_model.py:56: UserWarning: `model` does not have the `feature_names_in_`\n                attribute, so we cannot confirm that `model`'s feature\n                names match `X`'s column names. Proceed at your own\n                risk!\n                \n  warnings.warn(\n\n\n\nsource\n\n_Inspector.methods\n\n _Inspector.methods ()\n\nShow available methods.\n\ninspector.methods\n\n['_check_cols',\n 'permutation_importance',\n 'plot_feature_clusters',\n 'plot_partial_dependence',\n 'plot_permutation_importance',\n 'plot_pred_vs_act',\n 'plot_residuals',\n 'show_correlation']\n\n\n\nsource\n\n\n_Inspector.permutation_importance\n\n _Inspector.permutation_importance (sort:bool=True, scoring=None,\n                                    n_repeats=5, n_jobs=None,\n                                    random_state=None, sample_weight=None,\n                                    max_samples=1.0)\n\nCalculate permutation importance.\nParameters:\n\nsort: Sort features by decreasing importance.\n\nRemaining parameters are passed to sklearn.inspection._permutation_importance.permutation_importance.\n\ninspector.permutation_importance()\n\nbmi    0.482974\ns5     0.472974\nbp     0.137960\ns6     0.092853\ns3     0.088472\nage    0.077993\ns2     0.067045\ns1     0.051492\nsex    0.023468\ns4     0.022805\ndtype: float64\n\n\n\nsource\n\n\n_Inspector.plot_partial_dependence\n\n _Inspector.plot_partial_dependence (categorical_features=None,\n                                     feature_names=None, target=None,\n                                     response_method='auto', n_cols=3,\n                                     grid_resolution=100,\n                                     percentiles=(0.05, 0.95),\n                                     method='auto', n_jobs=None,\n                                     verbose=0, line_kw=None,\n                                     ice_lines_kw=None, pd_line_kw=None,\n                                     contour_kw=None, ax=None,\n                                     kind='average', centered=False,\n                                     subsample=1000, random_state=None)\n\nPlot partial dependence.\nReturns NumPy array of Axes objects.\nRemaining parameters are passed to sklearn.inspection._plot.partial_dependence.from_estimator.\n\naxes = inspector.plot_partial_dependence(features=[\"bp\", \"bmi\", [\"bp\", \"bmi\"]])\n\n\n\n\n\nsource\n\n\n_Inspector.plot_feature_clusters\n\n _Inspector.plot_feature_clusters (corr_method:str='spearman',\n                                   ax:matplotlib.axes._axes.Axes=None,\n                                   p=30, truncate_mode=None,\n                                   color_threshold=None, get_leaves=True,\n                                   orientation='top', labels=None,\n                                   count_sort=False, distance_sort=False,\n                                   show_leaf_counts=True, no_plot=False,\n                                   no_labels=False, leaf_font_size=None,\n                                   leaf_rotation=None,\n                                   leaf_label_func=None,\n                                   show_contracted=False,\n                                   link_color_func=None,\n                                   above_threshold_color='C0')\n\nPlot a dendrogram based on feature correlations.\nParameters:\n\ncorr_method: Method of correlation to pass to df.corr()\nax: Matplotlib Axes object. Plot will be added to this object if provided; otherwise a new Axes object will be generated.\n\nRemaining parameters are passed to model_inspector.explore.plot_column_clusters.\n\nax = inspector.plot_feature_clusters()\n\n\n\n\n\nsource\n\n\n_Inspector.plot_permutation_importance\n\n _Inspector.plot_permutation_importance\n                                         (ax:Optional[matplotlib.axes._axe\n                                         s.Axes]=None, importance_kwargs:O\n                                         ptional[dict]=None,\n                                         plot_kwargs:Optional[dict]=None)\n\nPlot a correlation matrix for self.X and self.y.\nParameters:\n\nax: Matplotlib Axes object. Plot will be added to this object if provided; otherwise a new Axes object will be generated.\nimportance_kwargs: kwargs to pass to sklearn.inspection.permutation_importance\nplot_kwargs: kwargs to pass to pd.Series.plot.barh\n\n\nax = inspector.plot_permutation_importance()\n\n\n\n\n\nsource\n\n\n_Inspector.show_correlation\n\n _Inspector.show_correlation (method='pearson', cmap:str|Colormap='PuBu',\n                              low:float=0, high:float=0, axis:Axis|None=0,\n                              subset:Subset|None=None,\n                              text_color_threshold:float=0.408,\n                              vmin:float|None=None, vmax:float|None=None,\n                              gmap:Sequence|None=None)\n\nShow a correlation matrix for self.X and self.y.\nIf output is not rendering properly when you reopen a notebook, make sure the notebook is trusted.\nRemaining parameters are passed to model_inspector.explore.show_correlation.\n\ninspector.show_correlation()\n\n\n\n\n\n\n \nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\nage\n1.00\n0.17\n0.19\n0.34\n0.26\n0.22\n-0.08\n0.20\n0.27\n0.30\n0.19\n\n\nsex\n0.17\n1.00\n0.09\n0.24\n0.04\n0.14\n-0.38\n0.33\n0.15\n0.21\n0.04\n\n\nbmi\n0.19\n0.09\n1.00\n0.40\n0.25\n0.26\n-0.37\n0.41\n0.45\n0.39\n0.59\n\n\nbp\n0.34\n0.24\n0.40\n1.00\n0.24\n0.19\n-0.18\n0.26\n0.39\n0.39\n0.44\n\n\ns1\n0.26\n0.04\n0.25\n0.24\n1.00\n0.90\n0.05\n0.54\n0.52\n0.33\n0.21\n\n\ns2\n0.22\n0.14\n0.26\n0.19\n0.90\n1.00\n-0.20\n0.66\n0.32\n0.29\n0.17\n\n\ns3\n-0.08\n-0.38\n-0.37\n-0.18\n0.05\n-0.20\n1.00\n-0.74\n-0.40\n-0.27\n-0.39\n\n\ns4\n0.20\n0.33\n0.41\n0.26\n0.54\n0.66\n-0.74\n1.00\n0.62\n0.42\n0.43\n\n\ns5\n0.27\n0.15\n0.45\n0.39\n0.52\n0.32\n-0.40\n0.62\n1.00\n0.46\n0.57\n\n\ns6\n0.30\n0.21\n0.39\n0.39\n0.33\n0.29\n-0.27\n0.42\n0.46\n1.00\n0.38\n\n\ntarget\n0.19\n0.04\n0.59\n0.44\n0.21\n0.17\n-0.39\n0.43\n0.57\n0.38\n1.00"
  }
]